%\documentclass[sigconf, authordraft]{acmart}
\documentclass[sigconf]{acmart}

\usepackage{booktabs} % For formal tables
\usepackage{tikz}
\usepackage{pgfbaselayers}
\usepackage[underline=false]{pgf-umlsd}
\usetikzlibrary{shadows}
\usetikzlibrary{arrows}

% Copyright
%\setcopyright{none}
%\setcopyright{acmcopyright}
%\setcopyright{acmlicensed}
\setcopyright{rightsretained}
%\setcopyright{usgov}
%\setcopyright{usgovmixed}
%\setcopyright{cagov}
%\setcopyright{cagovmixed}


% DOI
\acmDOI{10.1145/nnnnnnn.nnnnnnn}

% ISBN
\acmISBN{978-x-xxxx-xxxx-x/YY/MM}

% Conference
\acmConference[GECCO '19]{the Genetic and Evolutionary Computation Conference 2019}{July 13--17, 2019}{Prague, Czech Republic}
\acmYear{2019}
\copyrightyear{2019}

%\acmArticle{4}
\acmPrice{15.00}

% These commands are optional
%\acmBooktitle{Transactions of the ACM Woodstock conference}
%\editor{Jennifer B. Sartor}
%\editor{Theo D'Hondt}
%\editor{Wolfgang De Meuter}

\hypersetup{draft} 
\begin{document}

<<setup, cache=FALSE,echo=FALSE>>=
library(ggplot2)
library(ggthemes)
data.om <- read.csv('../data/gecco-2019-freqs-ap-threads.csv')
data.rr <- read.csv('../data/royal-road-evosoft.csv')
@

\title{Concurrency in evolutionary algorithms}

  \author{Anonymous Author 1}
  \orcid{1234-5678-9012}
  \affiliation{%
    \institution{Anonymous institute 1}
    \streetaddress{P.O. Box 1212}
    \city{Dublin}
    \state{Ohio}
    \postcode{43017-6221}
  }
  \email{aa1@ai1.com}
  
  \author{Anonymous Author 2}
  \affiliation{%
    \institution{Anonymous institute 2}
    \streetaddress{P.O. Box 1212}
    \city{Dublin}
    \state{Ohio}
    \postcode{43017-6221}
  }
  \email{2aa@ai2.com}

  \author{Anonymous Author 3}
  \affiliation{%
    \institution{Anonymous institute 2}
    \streetaddress{P.O. Box 1212}
    \city{Dublin}
    \state{Ohio}
    \postcode{43017-6221}
  }
  \email{2aa@ai2.com}

  \author{Anonymous Author 4}
  \affiliation{%
    \institution{Anonymous institute 2}
    \streetaddress{P.O. Box 1212}
    \city{Dublin}
    \state{Ohio}
    \postcode{43017-6221}
  }
  \email{2aa@ai2.com}
  
  % The default list of authors is too long for headers.
  \renewcommand{\shortauthors}{A. Author et al.}



\begin{abstract}
One of the ways multi-threaded concurrent algorithms are designed 
is around the concept of a channel, a structure
used to convey state to the different threads that are running
concurrently, which is why their design is one of the main challenges.
Concurrent evolutionary algorithms will use threads, communicating via
messages, for performing different types of tasks like evolving and
mixing populations. In this paper we will explore the design of these
messages in a communicating sequential processes context and with a 
concurrent language and how it influences scaling and the performance of the
algorithm itself. Eventually, we will show how a concurrent
version of algorithms offer a good option to leverage the
multi-threaded and multi-core capabilities of modern computers. In
order to do this, we will explore the performance of the system by
increasing the number of threads in order to find out the upper bounds
for the speed up and the causes why they are reached. Results show
that concurrency is achieved, but there the interaction between
communication, the number of threads and the algorithm parameters is
very important, opening new possibilities of algorithm design.

\end{abstract}

%
% The code below should be generated by the tool at
% http://dl.acm.org/ccs.cfm
% Please copy and paste the code instead of the example below. 
%
 \begin{CCSXML}
<ccs2012>
<concept>
<concept_id>10003752.10003809.10011778</concept_id>
<concept_desc>Theory of computation~Concurrent algorithms</concept_desc>
<concept_significance>500</concept_significance>
</concept>
<concept>
<concept_id>10010147.10010257.10010293.10011809.10011812</concept_id>
<concept_desc>Computing methodologies~Genetic algorithms</concept_desc>
<concept_significance>500</concept_significance>
</concept>
<concept>
<concept_id>10002944.10011123.10011674</concept_id>
<concept_desc>General and reference~Performance</concept_desc>
<concept_significance>300</concept_significance>
</concept>
</ccs2012>

\end{CCSXML}

\ccsdesc[500]{Theory of computation~Concurrent algorithms}
\ccsdesc[500]{Computing methodologies~Genetic algorithms}
\ccsdesc[300]{General and reference~Performance}

\keywords{Concurrency, Concurrent evolutionary algorithms, performance
evaluation, algorithm design}


\maketitle

\section{Introduction}

Despite the emphasis on leveraging newer hardware features
with best-suited software techniques
 there are not many papers \cite{Xia2010} dealing with 
creating concurrent evolutionary algorithms that work in a single
computing node or that extend seamlessly from single to many
computers. However, concurrent programming seems to be the best 
option if we are dealing with a multi-core processor architecture 
where many processes and threads can coexists at the same time. 
 This eventually means that many
processes (heavy or lightweight) can be leveraged to take full
advantage of the processor capabilities.

These capabilities must be matched at an abstract level by languages
that build on them so that high-level algorithms can use them without
worrying about the low-level mechanisms of creation or destruction of
threads, or how data is shared or communicated among them. These
languages are called concurrent, and the programming paradigm
implemented in them concurrency-oriented programming or simply
concurrent programming \cite{Armstrong2003}. 

These languages are characterized by the presence of
programming constructs that manage threads like first class
objects; that means that the language includes  operators for acting upon them and the
possibility of using them like parameters or function's result
values. This changes the coding of concurrent algorithms due to the
direct mapping between patterns of communications and processes with
% patterns of communication between processes? - Mario 
language expressions; on the one hand it becomes simpler since the
language provides an abstraction for communication, on the other hand
it changes the paradigm for implementing algorithms, since these new
communication constructs have to be taken into account. 

Moreover, concurrent programming adds a layer of abstraction over the parallel
facilities of processors and operating systems, offering a
high-level interface that allows the user to program modules of code to
be executed in parallel threads \cite{andrews1991concurrent}.

Different languages offer different concurrency strategies depending on how they deal with shared state,
that is, data structures that could be accessed from several processes or
threads. In this regard, there are two major fields and other, less well known models using, for instance, tuple
  spaces \cite{gelernter1985generative}:
\begin{itemize}
\item Actor-based concurrency \cite{schippers2009towards}
totally eliminates shared state by introducing a series of data
structures called {\em actors} that
store state and can mutate it locally. 
\item Process calculi or process algebra is a framework to describe
  systems that work with independent
  processes interacting between them using channels. One of the best
  known is called the {\em communicating sequential processes} (CSP)
methodology \cite{Hoare:1978:CSP:359576.359585}, which is effectively
stateless, with different processes reacting to a channel input without
changing state, and writing to these channels. Unlike actor based
concurrency, which keeps state local, in this case per-process state is totally
eliminated, with all computation state managed as messages in a channel.
\end{itemize}

Many modern languages, however, follow the CSP abstraction, and it has
become popular since it fits well other programming paradigms, like
reactive and functional programming, and allows for a more efficient
implementation, with less overhead, and with well-defined
primitives. This is why we will use it in this paper for creating  {\em natively}
concurrent evolutionary algorithms. We have chosen Perl 6, although 
other languages such as Go and Julia, are feasible alternatives.

In previous papers
\cite{Merelo:2018:MEA:3205651.3208317:anon,merelo:WEA:anon} we
designed an evolutionary algorithm that fits well this architecture
and explored its possibilities. That initial exploration showed that
a critical factor within this algorithmic model is the communication between threads; therefore designing
efficient messages is high-priority to obtain good algorithmic
performance and scaling. In this paper, we will test several
communication strategies: a lossless one that compresses the population,
and a lossy one that sends a representation of gene-wise
statistics of the population.

\section{State of the Art}

The problem of communication/synchronization
between processes or threads, has been the subject of long-standing study, 
in order to to fully leverage multiple processors 
on a given machine \cite{Lalwani2019}.

In the literature, most works are concerned with process-based  
concurrency, using for instance a Message Passing Interface (MPI) \cite{gropp1999using}.


% I think there could be confusion between the general "process" part 
% of an algorithm (from Process calculi), and an OS process (an instance of a program).a   
% So there is inter-process comunication/synchronization and
% inter-thread comunication/synchronization. For instance, Python can
% do a real parallel execution using processes and they can communicate 
% between them (RPC, RMI, Shared memory, message queues, etc.)
% Here, I think we are dealing only with communication between 
% threads. Python on the other hand cannot use two threads at the
% same time. - Mario

One of the 
best efforts to formalize and simplify that matter 
is Hoare's {\em Communicating Sequential Processes} \cite{Hoare:1978:CSP:359576.359585},
that proposes an interaction description language which is the 
theoretical support for many libraries and modern programming 
languages. In such model, concurrent programs communicate on 
the basis of {\em channels}, a sort of pipe buffer used to
interchange messages between the different processes or threads, 
either asynchronously or synchronously. 
Languages such as Go and Perl 6 implement this concurrency model 
as an abstraction for their multi-threading capabilities. 
(the latter including additional mechanisms such as 
{\em promises} or low-level access to the creation of threads).


The fact that messages have to be processed without secondary effects
and that actors do not share any kind of state makes concurrent
programming specially fit for functional languages or languages with
functional features; this has made this paradigm specially popular for
late cloud computing implementations; however, its presence in the EA
world is not so widespread, although some efforts have lately revived
the interest for this kind of paradigm \cite{swan2015research}.
Several years ago it was used in Genetic Programming
\cite{Briggs:2008:FGP:1375341.1375345,Huelsbergen:1996:TSE:1595536.1595579,walsh:1999:AFSFESIHLP}
and recently in neuroevolution \cite{Sher2013} but its occurrence in EA,
despite being scarce in the previous years
\cite{Hawkins:2001:GFG:872017.872197}, has experimented a certain rise
lately with papers such as \cite{valkov2018synthesis} which perform
program synthesis using the functional programming features of the Erlang language
\cite{barwell2017using} for building an evolutionary multi-agent system.

Regarding functional languages, Erlang and Scala have
embraced the actor model of concurrency and get excellent results in
many application domains; Clojure is another one with concurrent
features such as promises/futures, software transactional memory (STM) 
and agents; Kotlin \cite{simson2017open} has been recently used for
implementing a functional evolutionary algorithm framework.  

On the
other hand, Perl 6 \cite{Tang:2007:PRI:1190216.1190218,lenzperl} 
uses different concurrency models, varying from implicit concurrency 
using the {\tt hyper} and {\tt race} functions, that automatically 
parallelize operations on iterable data structures, to explicit 
concurrency using threads. 
The {\tt Algorithm::Evolutionary::Simple} library introduced last year
%\cite{DBLP:conf/gecco/GuervosV18}


Earlier efforts to study the issues of concurrency in EA are worth 
mentioning. For instance, the EvAg model \cite{evag:gpem} resorts to 
the underlying platform scheduler to manage the different threads of 
execution of the evolving agents; in this way the model scaled-up 
seamlessly to take full advantage of CPU cores. In the same avenue 
of measuring scalability,  experiments were conducted
in \cite{wcci:evoag} comparing single and a dual-core processor 
concurrency achieving near linear speed-ups . The latter
was later on extended in \cite{DBLP:conf/evoW/LaredoBMG12} by scaling
up the experiment to up to 188 parallel machines, reporting speed-ups 
up to $960\times$, nearly four times the expected linear growth 
in the number of machines (when local concurrency were not taken 
into account). Other authors have addressed explicitly multi-core 
architectures, such as Tagawa \cite{Tagawa201212} which used 
shared memory and a clever mechanism to avoid deadlocks. Similarly,
\cite{kerdprasop2012concurrent} used a message-based architecture 
developed in Erlang, separating GA populations as different
processes, although all communication was taking place with a common
central thread. 


In previous papers 
\cite{Merelo:2018:MEA:3205651.3208317:anon,Garcia-Valdez:2018:MEA:3205651.3205719:anon},
we presented a proof of concept of the implementation of a stateless 
evolutionary algorithms using Perl 6, based on a single 
channel model communicating threads for population evolving and 
mixing. In addition, we studied the effect of running parameters 
such as the {\em generation gap} (similar to the concept of {\em
time to migration} in parallel evolutionary algorithms) and 
population size, realizing that the choice of parameters may have 
a strong influence at the algorithmic level, but also at the 
implementation level, in fact affecting the actual wallclock
performance of the EA.

\section{Setting up the experiments using an evolutionary algorithm library in Perl 6}

Perl 6 is a concurrent, functional language
\cite{DBLP:journals/corr/abs-1809-01427} which was conceived with the
intention of providing a solid conceptual framework for multi-paradigm
computing, including thread-based concurrency and asynchrony. It's got
a heuristic optimization model, which literally optimizes code as it
runs. Its speed has been improving by a factor of 100x in the last few
years, with a performance that is in the same scale than interpreted
languages, although with some room for improvement.

The {\tt Algorithm::Evolutionary::Simple} Perl 6 module was published
in the ecosystem a year ago, and got recently into version 0.0.7. It
is a straightforward implementatin of a canonical evolutionary
algorithm with binary representation, and includes building blocks for
a generational genetic algorithm, as well as some fitness functions
used generally as benchmarks.

Those evolutionary algorithm building blocks do not include concurrent
primitives; it's up to the developer to design a concurrent
evolutionary algorithm using it.

\begin{figure}[h!tb]
  \centering
\includegraphics[width=0.95\columnwidth]{../figure/popmixer}
\caption{General scheme of operation of channels and thread groups. }
\label{fig:scheme}
\end{figure}
%

\begin{figure}[h!tb]
  \centering
  
\begin{sequencediagram}

\newthread[red]{E}{Evolver} 

\tikzstyle{inststyle}+=[rounded corners=3mm] 
\newinst{C}{Channel}

\tikzstyle{inststyle}+=[rounded corners=0]
\newthread[blue]{M}{Mixer}

\begin{call}{E}{evolve()}{E}{}\end{call}

\setthreadbias{east}
\begin{messcall}{E}{$pop_1$}{C} 
\mess{C}{$pop_1$}{M}
\end{messcall}

\prelevel\prelevel
\begin{call}{E}{evolve()}{E}{}\end{call}

\setthreadbias{east}
\begin{messcall}{E}{$pop_2$}{C}  
\mess{C}{$pop_2$}{M}
\end{messcall}

\prelevel\prelevel
\begin{call}{M}{mix()}{M}{}\end{call}

\postlevel\postlevel
\setthreadbias{west}
\begin{messcall}{M}{\shortstack{ \{\ $mixpop_1$,\\ $mixpop_2$,\\ \vdots \\ $mixpop_k$ \} }}{C}

\mess{C}{$mixpop_1$}{E} 
\begin{call}{E}{evolve()}{E}{}\end{call}

\setthreadbias{east}
\begin{messcall}{E}{$pop_3$}{C} 
\postlevel
\mess{C}{$mixpop_2$}{E} 
%\prelevel
\mess{C}{$pop_3$}{M}
\end{messcall}

\prelevel\prelevel
\begin{call}{E}{evolve()}{E}{}\end{call}

\setthreadbias{east}
\begin{messcall}{E}{$pop_4$}{C}
\mess{C}{$pop_4$}{M}
\end{messcall}

\setthreadbias{west}
\prelevel
\mess{C}{$mixpop_k$}{E}%\end{messcall}

\end{messcall}

\prelevel\prelevel
\begin{call}{M}{mix()}{M}{\vdots}\end{call}
\prelevel
\begin{call}{E}{evolve()}{E}{\vdots}\end{call}

\end{sequencediagram}

\caption{Schematic of communication between threads and channels for
  concurrent EAs. The two central bars represent the channel, and
  color corresponds to the {\em main} function they perform; blue for
  mixer, red for evolver. As the figure shows, the evolver threads
  always read from the mixer channel, and always write to the evolver
  channel.}
\label{fig:schematic}
\end{figure}


The baseline we are coming from is similar to the one used in previous experiments
\cite{Merelo:2018:MEA:3205651.3208317:anon}. Our intention was to
create a system that was not functionally equivalent to a sequential
evolutionary algorithms, and that followed the principle of
communicating sequential processes. In this kind of methodology, we
will have processes (or threads) communicating state through
channels. Every process itself will be stateless, reacting to the
presence of messages in the channels it is listening to and sending
result back to them, without changing state.

As in the previous papers, \cite{merelo:WEA:anon}, we will use two
groups of threads and two channels. We will see them in turns.

The two groups of threads perform the following
functions:\begin{itemize}
\item The {\em evolutionary} threads will be the ones that will be in
  principle running the evolutionary algorithm.
\item The {\em mixing} thread will {\em mix} populations, and create
  new ones as a mixture of them.
\end{itemize}

The two channels carry messages that are equivalent to populations,
but they do so in a different way:\begin{itemize}
  
\item The {\em evolutionary} channel will be used for carrying
  non-evolved, or generated, populations.
\item The {\em mixer} channel will carry, {\em in pairs}, evolved
  populations. 
\end{itemize}

These will be connected as shown in Figure \ref{fig:scheme}. The
evolutionary thread group will read only from the evolutionary channel,
evolve for a number of generations, and place result in the mixer
channel; the mixer group of threads will read only from the mixer
channel, in pairs. From every pair, a random element is put back into
the mixer channel, and a new population is generated and sent back to
the evolutionary channel. The main objective of using two channels is
to avoid deadlocks; the fact that one population is written always
back to the mixer channel avoids starvation in the channel. How this
runs in practice is shown in Figure \ref{fig:schematic}, where the
timeline of the interchange of messages between the evolver and mixer
threads and evolver and mixer channels is clarified.

The state of the algorithm will be transmitted via messages that
contain data about one population. Since using the whole population
will incur in a lob of overhead, we use a strategy that is similar to {\em EDA}, or estimation of distribution
  algorithm, whose basic idea is that the population message will
  contain the probability distribution over each gene. In this sense,
  this strategy is similar to the one presented by de la Ossa et
  al. in \cite{10.1007/978-3-540-30217-9_25}. Not being an estimation
  of distribution algorithm {\em per se}, since the evolutionary
  thread runs a canonical genetic algorithm, when the message is being
  composed, every (binary) gene of the 25\% best individuals in the
  population is examined, and an array with the
  probabilities for each gene is sent to the mixer thread. The {\em
    mixer} thread, in turn, just takes randomly one probability from
  each of the two {\em populations} (actually, distributions), instead
  of working on individuals. While in the baseline strategy the
  selection took place in the mixer thread, that eliminated half the
  population, in this case the selection takes place when composing
  the message, since just the 25\% best individuals are selected to
  compute the probability distribution of genes. When the evolver
  thread reads the message, it rebuilds the population based on this
  distribution.

\section{Experimental results}
\label{sec:exp}

We were mainly interested in the scaling capabilities of the algorithm
and implementation, so we tested several benchmark, binary functions:
OneMax, Royal Road and Leading Ones, all of them with 64 bits. 




\begin{figure*}[h!tb]
  \centering
\includegraphics[width=0.95\textwidth]{../figure/screenshot}
\caption{Screenshot using the htop utility of the used machine running
  two experiments at the same time. As it can be seen, all processors
  are kept busy, with a very high load average. }
\label{fig:screenshot}
\end{figure*}
%
However, the intention of concurrent evolutionary algorithms is to
leverage the power of all threads and processors in a computer so we must find out how it scales for different fitness functions. We are setting the number of initial populations was set to the
number of threads plus one, as the minimum required to avoid
starvation, and we are using a single mixing thread. As proved in our previous paper (hidden for anonimity), we are dividing the total population by the number of threads. The initial population will be 1024 for OneMax, 8192 for Royal Road and 4096 Leading Ones. These quantities were found heuristically by applying the bisection method on a selectorrecombinative algorith, which doubles the population until one that is able to find the solution 95\% of the time is found.

In this case, experiments were run on a different machine with the
Ubuntu 18.04 OS and an AMD Ryzen 7 2700X Eight-Core Processor
at 3.7GHz, which theoretically has 8 x 16 = 128 physical threads. Figure \ref{fig:screenshot} shows the utility htop with an 
experiment running; the top of the screen shows the rate at which all
cores are working, showing all of them occupied; of course, the
program was not running exclusively, but the list of processes below
show how the program is replicated in several processor, thus
leveraging their full power. Please check also the number of threads
that are actually running at the same time, a few of which are being used by our application; these are not, however, physical but operating system threads; the OS is able to accomodate many more threads that are physically available if the code using them is idle. 

\begin{figure}[h!tb]
  \centering
<<threads1, cache=FALSE,echo=FALSE,fig.height=4,message=FALSE>>=
data.om <- read.csv('../data/gecco-2019-freqs-ap-threads.csv')
ggplot(data.om,aes(x=Threads,y=Evaluations,group=Threads)) + geom_boxplot(fill='green',notch=TRUE)+theme_tufte()+scale_x_continuous(name="Threads",breaks=c(2,4,6,8))+labs(x="Threads",y="Evaluations",title="Scaling")
@ 
\caption{Number of evaluations vs. number of threads. Higher is better.}
\label{fig:threads1}
\end{figure}
%
\begin{figure}[h!tb]
  \centering
<<threads2, cache=FALSE,echo=FALSE,fig.height=4,message=FALSE>>=
ggplot(data.om,aes(x=Threads,y=Time,group=Threads))+geom_boxplot(fill='blue',notch=TRUE)+theme_tufte()+scale_x_continuous(name="Threads",breaks=c(2,4,6,8))+labs(x="Generation.Gap",y="Time",title="Scaling")
@ 
\caption{Total time vs. number of threads. Lower is better.}
\label{fig:threads2}
\end{figure}
%
We are first interested in the number of evaluations needed to find
the solution, which are plotted in Fig. \ref{fig:threads1}. It
increases slightly and not significantly from 2 to 4 threads, but it
does increase significantly for 6 and 8 threads, indicating that the
algorithm's performance  is worse when we increase the number
of threads. This is probably due to the fact that we are
simultaneously decreasing the population size, leading to earlier
convergence for the number of generations (8) it is being used. This
interplay between the degree of concurrency, the population size and
the number of generations will have to be explored further.

\begin{figure}[h!tb]
  \centering
<<threads3, cache=FALSE,echo=FALSE,fig.height=4,message=FALSE>>=
ggplot(data.om,aes(x=Threads,y=Evaluations/Time,group=Threads))+geom_boxplot(fill='yellow',notch=TRUE)+theme_tufte()+scale_x_continuous(name="Threads",breaks=c(2,4,6,8))+labs(x="Generation.Gap",y="Evaluation/time",title="Scaling")
@ 
\caption{Evaluations per second vs. number of threads. Higher is better.}
\label{fig:threads3}
\end{figure}
%
But we were also interested in the actual wallclock time, plotted in
Fig. \ref{fig:threads2}. The picture shows that it decreases
significantly when we go from 2 (the baseline) to 4 threads, since we
are using more computing power for (roughly) the same number of
evaluations. It then increases slightly we we increase the number of
threads; as a matter of fact and as shown in \ref{fig:threads3}, the
number of evaluations per second increases steeply up to 6 threads,
and slightly when we use 8 threads. However, the amount of evaluations
needed overcompensates this speed, resulting in a worse results. It
confirms, however, that we are actually using all threads for
evaluations, and if only we could find a strategy that didn't need
more evaluations we should be able to get a big boost in computation
time that scales gracefully to a high number of processors.


\section{Conclusions}
\label{sec:conclusions}

Designing a concurrent, stateless evolutionary algorithm brings a new
set of configuration decisions that must be taken into account at the algorithm and
at the parameter level. Thus, the main intention in this paper was to
explore the parameter space in a concurrent evolutionary algorithm
looking for the combination that yields the best performance in terms
of time, without sacrificing the algorithmic performance. 

Finally, this paper has shown for the first time the good scaling
behavior of this kind of concurrent evolutionary
algorithms. Simultaneous threads running an evolutionary algorithm do
increase the number of simultaneous evaluations, although since this
scaling is achieved via population splitting, the actual time achieved
reaches its lowest peak with just 4 threads. As a matter of fact, and
probably due to hardware limitations, the number of simultaneous
evaluations seems to reach a plateau with 8 threads. At any rate, this
new concurrent evolutionary algorithm is achieving results that are
much better than what the equivalent, single thread, evolutionary
algorithm would achieve. It should be noted that the base algorithm
used for comparison uses 3 OS threads already: two evolutionary threads
and a mixing thread; thus, the scaled-up version would use a total of
9 threads (8 evolutionary + 1 for mixing).

Although the good performance of this concurrent evolutionary
algorithm has been well established in this paper, several lines of
research are open. The first one is to continue exploring the
algorithm itself trying to find out a scaling strategy that does not
have a negative influence in the number of evaluations, so that actual
increasing in the number of simultaneous evaluations achieved the
equivalent speedup. The second one would be to find out what is the
maximum number of simultaneous evaluations that can be achieved, so
that an optimum number of threads can be advised, if possible
independently of the problem and depending only on the physical number
of cores available.

The messaging strategies proposed here can be used only if the problem
can be represented via a binary data structure. New strategies will
have to be explored for floating-point representation, or more
complicated data structures. There are general-purpose strategies for
compressing messages, for instance, and they could be used in this
case. Estimation of distribution algorithms can also be extended to
other types of data, so this is is something that can be also explored.

Finally, we are using the same kind of algorithm in all
threads. Nothing prevents us from using a different algorithm per
thread, or using different parameters per thread. This opens a vast
space of possibilities, but the payoff might be worth it.



\begin{acks}
Acknowledgements taking\\
this much\\
space

\end{acks}

\bibliographystyle{ACM-Reference-Format}
\bibliography{../geneura,../concurrent,../perl6} 

\end{document}
