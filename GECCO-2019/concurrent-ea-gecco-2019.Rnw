%\documentclass[sigconf, authordraft]{acmart}
\documentclass[sigconf]{acmart}

\usepackage{booktabs} % For formal tables
\usepackage{tikz}
\usepackage{pgfbaselayers}
\usepackage[underline=false]{pgf-umlsd}
\usetikzlibrary{shadows}
\usetikzlibrary{arrows}

% Copyright
%\setcopyright{none}
%\setcopyright{acmcopyright}
%\setcopyright{acmlicensed}
\setcopyright{rightsretained}
%\setcopyright{usgov}
%\setcopyright{usgovmixed}
%\setcopyright{cagov}
%\setcopyright{cagovmixed}


% DOI
\acmDOI{10.1145/nnnnnnn.nnnnnnn}

% ISBN
\acmISBN{978-x-xxxx-xxxx-x/YY/MM}

% Conference
\acmConference[GECCO '19]{the Genetic and Evolutionary Computation Conference 2019}{July 13--17, 2019}{Prague, Czech Republic}
\acmYear{2019}
\copyrightyear{2019}

%\acmArticle{4}
\acmPrice{15.00}

% These commands are optional
%\acmBooktitle{Transactions of the ACM Woodstock conference}
%\editor{Jennifer B. Sartor}
%\editor{Theo D'Hondt}
%\editor{Wolfgang De Meuter}

\hypersetup{draft} 
\begin{document}

<<setup, cache=FALSE,echo=FALSE>>=
library(ggplot2)
library(ggthemes)

data.freqs.nw.gens <- read.csv('../data/gecco-2019-freqs-noweb-generations.csv')
data.compress.nw.gens <- read.csv('../data/gecco-2019-compress-noweb-generations.csv')
data.freqs.generations <- read.csv('../data/gecco-2019-freqs-generations.csv')
data.compress.generations <- read.csv('../data/gecco-2019-compress-generations.csv')
data.evostar <- read.csv("../data/evostar2019.csv")

data.generations <- data.frame(
    strategy=c(rep("Compress",length(data.compress.generations$Generation.Gap)),
               rep("CompressNW",length(data.compress.nw.gens$Generation.Gap)),
               rep("EDA",length(data.freqs.generations$Generation.Gap)),
               rep("EDANW",length(data.freqs.nw.gens$Generation.Gap)),
               rep("Full",length(data.evostar$gap))),
    gap=c(data.compress.generations$Generation.Gap,data.compress.nw.gens$Generation.Gap,data.freqs.generations$Generation.Gap,data.freqs.nw.gens$Generation.Gap,data.evostar$gap),
    evaluations=c(data.compress.generations$Evaluations,data.compress.nw.gens$Evaluations,data.freqs.generations$Evaluations,data.freqs.nw.gens$Evaluation,data.evostar$evaluations),
    time=c(data.compress.generations$Time,data.compress.nw.gens$Time,data.freqs.generations$Time,data.freqs.nw.gens$Time,data.evostar$time) )

@

\title{Exploring concurrent and stateless evolutionary algorithms}
\subtitle{Improving the algorithmic efficiency and performance of channel-based evolutionary algorithms}
% Same title as EvoApp paper? - Juanlu
% I propose that the subtitle becomes the title and the subtitle becomes something like: "The case of a Perl6-based implementation" -  Juanlu

  \author{Anonymous Author 1}
  \orcid{1234-5678-9012}
  \affiliation{%
    \institution{Anonymous institute 1}
    \streetaddress{P.O. Box 1212}
    \city{Dublin}
    \state{Ohio}
    \postcode{43017-6221}
  }
  \email{aa1@ai1.com}
  
  \author{Anonymous Author 2}
  \affiliation{%
    \institution{Anonymous institute 2}
    \streetaddress{P.O. Box 1212}
    \city{Dublin}
    \state{Ohio}
    \postcode{43017-6221}
  }
  \email{2aa@ai2.com}

  \author{Anonymous Author 3}
  \affiliation{%
    \institution{Anonymous institute 2}
    \streetaddress{P.O. Box 1212}
    \city{Dublin}
    \state{Ohio}
    \postcode{43017-6221}
  }
  \email{2aa@ai2.com}

  \author{Anonymous Author 4}
  \affiliation{%
    \institution{Anonymous institute 2}
    \streetaddress{P.O. Box 1212}
    \city{Dublin}
    \state{Ohio}
    \postcode{43017-6221}
  }
  \email{2aa@ai2.com}
  
  % The default list of authors is too long for headers.
  \renewcommand{\shortauthors}{A. Author et al.}

  

\begin{abstract}
Concurrent algorithms use channels for communication, which implies
that communication is an integral part of them.  Therefore, when designing a concurrent algorithm some attention must be devoted to the layout of the communication
architecture in order to gain performance and achieve an efficient 
 design.
Concurrent evolutionary algorithms
will use threads for performing different types of tasks like evolving
and mixing populations and these threads communicate via messages. In
this paper we will explore the design of these messages and how it
influences scaling and the performance of the algorithm itself. The
evolutionary algorithm is implemented using a concurrent 
programming language in order to fully exploit the algorithm's capabilities.
Eventually, we will show
how concurrent version of algorithms 
offer a good option to leverage the
multi-threaded and multi-core capabilities of modern computers.
In order to do this, we will explore the performance of the system by
increasing the number of threads in order to find out the upper bounds for
the speed up and the causes why they are reached. Results show that
concurrency is achieved, but there the interaction between
communication, the number of threads and the algorithm parameters is
very important, opening new possibilities of algorithm design.
% @JJ: After reading this last line, I definetely think that we should
% go for a sensitivity analysis. - Juanlu
% Results show that an efficient concurrency can be achieved, but there
% 

\end{abstract}

%
% The code below should be generated by the tool at
% http://dl.acm.org/ccs.cfm
% Please copy and paste the code instead of the example below. 
%
 \begin{CCSXML}
<ccs2012>
<concept>
<concept_id>10003752.10003809.10011778</concept_id>
<concept_desc>Theory of computation~Concurrent algorithms</concept_desc>
<concept_significance>500</concept_significance>
</concept>
<concept>
<concept_id>10010147.10010257.10010293.10011809.10011812</concept_id>
<concept_desc>Computing methodologies~Genetic algorithms</concept_desc>
<concept_significance>500</concept_significance>
</concept>
<concept>
<concept_id>10002944.10011123.10011674</concept_id>
<concept_desc>General and reference~Performance</concept_desc>
<concept_significance>300</concept_significance>
</concept>
</ccs2012>

\end{CCSXML}

\ccsdesc[500]{Theory of computation~Concurrent algorithms}
\ccsdesc[500]{Computing methodologies~Genetic algorithms}
\ccsdesc[300]{General and reference~Performance}

\keywords{Concurrency, Concurrent evolutionary algorithms, performance
evaluation, algorithm design}


\maketitle

\section{Introduction}


Despite the emphasis on algorithmic techniques whose foundation is on
software techniques that have an ultimate basis on newer hardware
features, there are not many papers \cite{Xia2010} dealing with 
creating concurrent evolutionary algorithms that work in a single
computing node or that extend seamlessly from single to many
computers. Concurrent programming has a physical basis: the existence
of multi-core processors that are able to deal with many processes at
the same time, and the support for threads, or lightweight processes,
within those same processors. This eventually means that many
processes (heavy or lightweight) can be leveraged to take full
advantage of the processor capabilities.

These capabilities must be matches at an abstract level by languages
that build on them so that high-level algorithms can use them without
worrying about the low-level mechanisms of creation or destruction of
threads, or how data is shared or communicated among them. These
languages are called concurrent, and the programming paradigm
implemented in them concurrency-oriented programming or simply
concurrent programming \cite{Armstrong2003}. 

These languages are characterized by the presence of
programming constructs that manage processes like first class
objects; that means that the language includes  operators for acting upon them and the
possibility of using them like parameters or function's result
values. This changes the coding of concurrent algorithms due to the
direct mapping between patterns of communications and processes with
language expressions; on one hand it becomes simpler since the
language provides an abstraction for communication, on the other hand
it changes the paradigm for implementing algorithms, since these new
communication constructs have to be taken into account. 

Moreover, concurrent programming adds a layer of abstraction over the parallel
facilities of processors and operating systems, offering a
high-level interface that allows the user to program modules of code to
be executed in parallel threads \cite{andrews1991concurrent}.

Different languages offer different concurrency strategies depending on how they deal with shared state,
that is, data structures that could be accessed from several
processes. In this regard, there are two major fields (with some other
variations): 
\begin{itemize}
\item Actor-based concurrency \cite{schippers2009towards}
totally eliminates shared state by introducing a series of data
structures called {\em actors} that
store state and can mutate it locally. 
\item Process calculi or process algebra is a framework to describe
  systems that work with independent
  processes that interact between them using channels. One of the best
  known is called the {\em communicating sequential processes} (CSP)
methodology \cite{Hoare:1978:CSP:359576.359585}, which is effectively
stateless, with different processes reacting to a channel input without
changing state, and writing to these channels. Unlike actor based
concurrency, which keeps state local, in this case per-process state is totally
eliminated, with all computation state managed as messages in a channel.
\item Other, less well known models using, for instance, tuple
  spaces \cite{gelernter1985generative}.
\end{itemize}

Most modern languages, however, follow the CSP abstraction, and it has
become popular since it fits well other programming paradigms, like
reactive and functional programming, and allows for a more efficient
implementation, with less overhead, and with well-defined
primitives. This is why we will use it in this paper for creating new
evolutionary algorithms that live {\em natively} in these environments
and can thus be implemented easily in this kind of languages. We have chosen Perl 6, although 
other languages such as Go, are feasible alternatives.

In previous papers
\cite{Merelo:2018:MEA:3205651.3208317:anon,merelo:WEA:anon} we
designed an evolutionary algorithm that fits well this architecture
and explored its possibilities. That initial exploration showed that
a critical factor within this algorithmic model is communication between threads; therefore designing
efficient messages is high-priority to obtain a good algorithmic
performance and scaling. In this paper, we will test several
communication strategies: a lossless one that compresses the population,
and a lossy one that sends a representation of population gene-wise
statistics.

\section{State of the Art}

The problem of communication/synchronization
between processes, which nowadays is accomplished by means of threads, has been the subject of long-standing study. One of the best efforts to formalize
and simplify that matter is Hoareâ€™s {\em Communicating Sequential
  Processes} \cite{Hoare:1978:CSP:359576.359585}, that proposes an interaction
description language which is the theoretical support for many libraries and
modern programming languages. In such model, concurrent programs communicate on the basis of {\em channels}, a sort of pipe buffer used to interchange messages between the
different processes or threads, either asynchronously or synchronously. Languages such as Go and Perl 6 implement this concurrency model (the latter including additional mechanisms such as {\em promises} or low-level access to the creation of threads).

% SR: I suggest to eliminate the paragraph below, as it has been covered in the Introduction section

%, this one. Another, different, approach is actor-based concurrency, \cite{schippers2009towards}. This actor model bans shared state, with different {\em actors} communicating through messages \cite{erb2012concurrent}. 


The fact that messages have to be processed without secondary effects
and that actors do not share any kind of state makes concurrent
programming specially fit for functional languages or languages with
functional features; this has made this paradigm specially popular for
late cloud computing implementations; however, its presence in the EA
world is not so widespread, although some efforts have lately revived
the interest for this kind of paradigm \cite{swan2015research}. Several years ago it was used in Genetic Programming
\cite{Briggs:2008:FGP:1375341.1375345,Huelsbergen:1996:TSE:1595536.1595579,walsh:1999:AFSFESIHLP}
and recently in neuroevolution \cite{Sher2013} but its occurrence in EA,
despite being scarce in the previous years
\cite{Hawkins:2001:GFG:872017.872197}, has experimented a certain rise
lately with papers such as \cite{valkov2018synthesis} which perform
program synthesis using the functional programming features of the Erlang language
\cite{barwell2017using} for building an evolutionary multi-agent system.

Regarding functional languages, Erlang and Scala have
embraced the actor model of concurrency and get excellent results in
many application domains; Clojure is another one with concurrent
features such as promises/futures, Software Transaction Memory and
agents; Kotlin \cite{simson2017open} has been recently used for
implementing a functional evolutionary algorithm framework.  

On the
other hand, Perl 6 \cite{Tang:2007:PRI:1190216.1190218,lenzperl} uses different
concurrency models, varying from implicit concurrency using a
particular function that automatically parallelizes operations on
iterable data structures, to explicit concurrency using threads. These
both types of concurrency will be considered in this study whilst using 
the {\tt Algorithm::Evolutionary::Simple}, a Perl 6 library introduced in last year's version of this very same conference \cite{DBLP:conf/gecco/GuervosV18}.

%%% SR: Two paragraphs below are replicated from Evo* paper. I rewrote/summarised them

% For instance, the EvAg model \cite{evag:gpem} is a locally concurrent and globally
% parallel evolutionary algorithm that leaves the management of the
% different agents (i.e. threads) to the underlying platform scheduler
% and displays an interesting feature: the model is able to scale
% seamlessly and take full advantage of CPU threads. In a first attempt
% to measure the scalability of the approach experiments were conducted
% in \cite{wcci:evoag} for a single and a dual-core processor showing
% that, for cost functions passing some milliseconds of computing
% time, the model was able to achieve near linear speed-ups . This study
% was later on extended in \cite{DBLP:conf/evoW/LaredoBMG12} by scaling
% up the experimentation to up to 188 parallel machines. The
% reported speed-up was $\times 960$ which is beyond the linear $\times
% 188$ that could be expected if local concurrency were not taken into
% account.  
% The aforementioned algorithm used a protocol that worked
% asynchronously, leveraging its peer-to-peer capabilities; in general
% the design of concurrent EAs has to take into account the
% communication/synchronization 
% between processes, which nowadays will be mainly threads. Although the
% paper above was original in its approach, other authors targeted
% explicitly multi-core architectures, such as Tagawa
% \cite{Tagawa201212} which used shared memory and a clever mechanism to
% avoid deadlock. Other authors \cite{kerdprasop2012concurrent} actually
% use a message-based architecture based in the concurrent functional
% language Erlang, which separates GA populations as different
% processes, although all communication takes place with a common
% central thread. 

Earlier efforts to study the issues of concurrency in EA are worth mentioning. For instance, the EvAg model \cite{evag:gpem} resorts to the underlying platform scheduler to manage the different threads of execution of the evolving agents; in this way the model scaled-up 
seamlessly to take full advantage of CPU cores. In the same avenue of measuring scalability,  experiments were conducted
in \cite{wcci:evoag} comparing single and a dual-core processor concurrency 
achieving near linear speed-ups . The latter
was later on extended in \cite{DBLP:conf/evoW/LaredoBMG12} by scaling
up the experiment to up to 188 parallel machines, reporting speed-ups up to $960\times$, nearly four times the expected linear growth in the number of machines (when local concurrency were not taken into account). Other authors have addressed
explicitly multi-core architectures, such as Tagawa
\cite{Tagawa201212} which used shared memory and a clever mechanism to
avoid deadlock. Similarly, \cite{kerdprasop2012concurrent}
used a message-based architecture developed in Erlang, separating GA populations as different
processes, although all communication taking place with a common
central thread. 

%%% SR: Again, below paragraphs replicated in Evo*. I re-wrote/summarised

% In our previous papers 
% \cite{Merelo:2018:MEA:3205651.3208317:anon,Garcia-Valdez:2018:MEA:3205651.3205719:anon},
% we presented the proof of concept and initial results with this kind
% of stateless evolutionary algorithms, implemented in the Perl 6
% language. These evolutionary algorithms use a single channel where
% whole populations are sent. The (stateless) functions read a single
% population from the channel, run an evolutionary algorithm for a fixed
% number of generations, which we call the {\em generation gap} or
% simply {\em gap}, and send the population in the final generation back
% to the channel. Several populations are created initially, and a
% concurrent {\em mixer} is run which takes populations in couples,
% mixes them leaving only a single population with the best individuals
% selected from the two merged populations.
%  This {\em
%   gap} is then conceptually, if not functionally, similar to the {\em
%   time to migration} in parallel evolutionary algorithms (with which
% concurrent evolutionary algorithms have a big resemblance).
% 
% We did some initial exploration of the parameter space in
% \cite{merelo:WEA:anon}. In these initial explorations we realized
% that the parameters we used had an influence at the algorithmic level,
% but also at the implementation level, changing the wallclock
% performance of the algorithm.

In previous papers 
\cite{Merelo:2018:MEA:3205651.3208317:anon,Garcia-Valdez:2018:MEA:3205651.3205719:anon},
we presented a proof of concept of the implementation 
of stateless evolutionary algorithms using Perl 6, based on a single channel model communicating threads for population evolving and mixing. In addition, we studied the effect of running parameters such as {\em generation gap} (similar to the concept of {\em
  time to migration} in parallel evolutionary algorithms) and population size, realizing that the choice of parameters may have a strong influence at the algorithmic level,
but also at the implementation level, in fact affecting the actual wallclock
performance of the EA.

%%% SR: I suggest to eliminate the paragraph below

% In this paper we will explore the parameter space systematically
% looking particularly at two parameters that have a very important
% influence on performance: population size and generation gap. Our
% intention is to create a rule of thumb for setting them in this kind
% of algorithms, so that they are able to achieve the best
% performance. 
% 
% We will present the experimental setup next.

\section{Experimental setup}
\label{sec:exp}

\begin{figure}[h!tb]
  \centering
\includegraphics[width=0.95\columnwidth]{../figure/popmixer}
\caption{General scheme of operation of channels and thread groups. }
\label{fig:scheme}
\end{figure}
%

\begin{figure}[h!tb]
  \centering
  
\begin{sequencediagram}

\newthread[red]{E}{Evolver} 

\tikzstyle{inststyle}+=[rounded corners=3mm] 
\newinst{C}{Channel}

\tikzstyle{inststyle}+=[rounded corners=0]
\newthread[blue]{M}{Mixer}

\begin{call}{E}{evolve()}{E}{}\end{call}

\setthreadbias{east}
\begin{messcall}{E}{$pop_1$}{C} 
\mess{C}{$pop_1$}{M}
\end{messcall}

\prelevel\prelevel
\begin{call}{E}{evolve()}{E}{}\end{call}

\setthreadbias{east}
\begin{messcall}{E}{$pop_2$}{C}  
\mess{C}{$pop_2$}{M}
\end{messcall}

\prelevel\prelevel
\begin{call}{M}{mix()}{M}{}\end{call}

\postlevel\postlevel
\setthreadbias{west}
\begin{messcall}{M}{\shortstack{ \{\ $mixpop_1$,\\ $mixpop_2$,\\ \vdots \\ $mixpop_k$ \} }}{C}

\mess{C}{$mixpop_1$}{E} 
\begin{call}{E}{evolve()}{E}{}\end{call}

\setthreadbias{east}
\begin{messcall}{E}{$pop_3$}{C} 
\postlevel
\mess{C}{$mixpop_2$}{E} 
%\prelevel
\mess{C}{$pop_3$}{M}
\end{messcall}

\prelevel\prelevel
\begin{call}{E}{evolve()}{E}{}\end{call}

\setthreadbias{east}
\begin{messcall}{E}{$pop_4$}{C}
\mess{C}{$pop_4$}{M}
\end{messcall}

\setthreadbias{west}
\prelevel
\mess{C}{$mixpop_k$}{E}%\end{messcall}

\end{messcall}

\prelevel\prelevel
\begin{call}{M}{mix()}{M}{\vdots}\end{call}
\prelevel
\begin{call}{E}{evolve()}{E}{\vdots}\end{call}

\end{sequencediagram}

\caption{Schematic of communication between threads and channels for
  concurrent EAs. The two central bars represent the channel, and
  color corresponds to the {\em main} function they perform; blue for
  mixer, red for evolver. As the figure shows, the evolver threads
  always read from the mixer channel, and always write to the evolver
  channel.}
\label{fig:schematic}
\end{figure}


The baseline we are coming from is similar to the one used in previous experiments
\cite{Merelo:2018:MEA:3205651.3208317:anon}. Our intention was to
create a system that was not functionally equivalent to a sequential
evolutionary algorithms, and that followed the principle of
communicating sequential processes. In this kind of methodology, we
will have processes (or threads) communicating state through
channels. Every process itself will be stateless, reacting to the
presence of messages in the channels it is listening to and sending
result back to them, without changing state.

As in the previous papers, \cite{merelo:WEA:anon}, we will use two
groups of threads and two channels. We will see them in turns.

The two groups of threads perform the following
functions:\begin{itemize}
\item The {\em evolutionary} threads will be the ones that will be in
  principle running the evolutionary algorithm.
\item The {\em mixing} threads will {\em mix} populations, and create
  new ones as a mixture of them.
\end{itemize}

The two channels carry messages that are equivalent to populations,
but they do so in a different way:\begin{itemize}
  
\item The {\em evolutionary} channel will be used for carrying
  non-evolved, or generated, populations.
\item The {\em mixer} channel will carry, {\em in pairs}, evolved
  populations. 
\end{itemize}

These will be connected as shown in Figure \ref{fig:scheme}. The
evolutionary group of threads will read only from the evolutionary channel,
evolve for a number of generations, and place result in the mixer
channel; the mixer group of threads will read only from the mixer
channel, in pairs. From every pair, a random element is put back into
the mixer channel, and a new population is generated and sent back to
the evolutionary channel. The main objective of using two channels is
to avoid deadlocks; the fact that one population is written always
back to the mixer channel avoids starvation in the channel. How this
runs in practice is shown in Figure \ref{fig:schematic}, where the
timeline of the interchange of messages between the evolver and mixer
threads and evolver and mixer channels is clarified.

One of the problems of the baseline configuration was that
communication took a great amount of time, adding some overhead to the
algorithm. The {\em message} consisted of the whole population, and
the size increased with population size, obviously. This tipped the
balance between communication and computation towards communication,
so that the more threads, the more communication was taking place. Our
first intention in this paper was to slim down messages so that they
took less bandwidth (or memory) and less time to send and process. In
general, this strategy also can be framed in the context of migration
strategies, since that is the most similar thing in the context of
parallel algorithms. In parallel algorithms, an adequate selection of
migration strategies, balancing exploration and exploitation, is the
key to achieving high performance, as indicated in
\cite{Cantu-Paz:1999:MPT:2933923.2934003}.
In the context of concurrent evolutionary algorithms we will talk
about {\em population messages}, but their effect is going to be
similar. For this paper, we introduced two different messaging strategies: \begin{itemize}
  
\item One we have called {\em EDA}, or estimation of distribution
  algorithm, whose basic idea is that the population message will
  contain the probability distribution over each gene. In this sense,
  this strategy is similar to the one presented by de la Ossa et
  al. in \cite{10.1007/978-3-540-30217-9_25}. Not being an estimation
  of distribution algorithm {\em per se}, since the evolutionary
  thread runs a canonical genetic algorithm, when the message is being
  composed, every (binary) gene of the 25\% best individuals in the
  population is examined, and an array with the
  probabilities for each gene is sent to the mixer thread. The {\em
    mixer} thread, in turn, just takes randomly one probability from
  each of the two {\em populations} (actually, distributions), instead
  of working on individuals. While in the baseline strategy the
  selection took place in the mixer thread, that eliminated half the
  population, in this case the selection takes place when composing
  the message, since just the 25\% best individuals are selected to
  compute the probability distribution of genes. When the evolver
  thread reads the message, it rebuilds the population based on this
  distribution.
\item The second is called {\em compress}, and it simply bit-packs the
  population, without the fitness, into a message which uses 1 bit per
  individual, and then 64 bits, or simply 8 bytes, to transmit a
  single individual in the population. This strategy is equivalent to
  the baseline, except it introduces an additional step of evaluating
  the population when mixing and receiving it from the evolutionary
  channel. It is hoped that this additional evaluation overhead does
  compensates the communication overhead that is eliminated.
\end{itemize}


In the same way we did in our previous papers, first we will have to
evaluate these new strategies compared with baseline; since the
overhead will be different depending on the computation time, which in
this case is regulated by the number of generations that are going to
be performed by every thread, we will first perform an experiment
changing that. We will call this parameter the {\em generation gap},
implying that's the gap between receiving a message and activating the
thread and sending it, deactivating it. Besides, what we want to find
out in these set of experiments is what is the 
generation gap that gives the best performance in terms of raw time to
find a solution, as well as the best number of evaluations per
second.

Additionally, it is impossible to know from first principles if this
setup is the only possible. We have to heuristically explore other
possibilities; in this case, we will explore another messaging
strategy called {\em no writeback}, or {\sf nw}, where the mixer
thread, instead of sending one of the individuals back again to the
mixer thread, sends it to the evolver channel, where it will undergo
an additional round of evolution. The main difference between these
two strategies is twofold:\begin{itemize}
\item The mixer channel can be empty for some time, since it is not
  always holding at least one population message (written back every
  time it is activated). This might lead to {\em starvation} of the
  mixer thread, but in fact it will not take long since it is going to
  be processed immediately by the evolver thread.
\item Every population is mixed just once, which might lead to
  improvements in the algorithm.
\end{itemize}


\section{Experimental results}
\label{sec:exp}

The experiments have been prepared by using OneMax function with 64
bits. This function was chosen primarily since it was the one used in
previous experiments, but also because it is a classical benchmark, it
can be easily programmed in Perl 6, and allowed us to focus in what we
are interested in, the design of the concurrent evolutionary algorithm
itself. We used the open source {\tt Algorithm::Evolutionary::Simple}
Perl 6 module for the evolutionary part, and wrote the different
scripts in the same language. The latest version, compiled from
source, of Perl 6 was used, and experiments were performed in a
Intel(R) Core(TM) i7-4770 CPU at 3.40GHz running Ubuntu 14.04 server.
All scripts have a free license and have been released in GitHub,
where the data generated by every single experiment is also hosted.

\begin{table}
  \centering
  \caption{Parameters used to explore the generation gap}
  \label{tab:gens}
  \begin{tabular}{lr}
    \toprule
    Parameter & Value\\
    \midrule
Evolver threads & 2 \\
    Mixer threads & 1 \\
    Generations & 8,16,32 \\
    Population size & 256 \\
    Initial populations & 3 \\
    Bits & 64 \\
    Repetitions & $>=$ 15 \\
  \bottomrule
  \end{tabular}
\end{table}
%
In these experiments, the parameters used are shown in Table
\ref{tab:gens}. Two evolver threads are needed, at least, to avoid
starvation of the mixer thread. The generation gap was checked for
those values in all cases, although in some cases we extended it to 4
and 64 generations. The population was sized in previous papers using
the bisection method, and the number of initial populations created
and sent to the evolver channel was also designed to avoid starvation;
that way, as soon as the first two populations are evaluated
simultaneously by the two threads and sent to the mixer channel, this
channel will always hold a population that will combine with a fresh
one coming from either of the mixer threads.

\begin{figure*}[h!tb]
  \centering
<<gens, cache=FALSE,echo=FALSE,fig.height=4>>=
ggplot(data.generations,aes(x=gap,y=evaluations,group=strategy, color=strategy))+ stat_summary(fun.y="mean", geom="line", size=2)+geom_point()+scale_y_log10()+scale_x_continuous(trans='log10',name="Generations",breaks=c(4,8,16,32,64))+theme_tufte()+labs(x="Generations",y="Evaluations")
@ 
\caption{Number of evaluations vs. generation gap for the baseline
  strategy (in pink) and the EDA and compress messaging strategies,
  with or without writeback (WB). Lines run over the average value;
  every point represents the number of evaluations in individual
  experiments. Smaller, in this case, is better. Please note that both
axes are logarithmic.}
\label{fig:evals}
\end{figure*}
%
\begin{figure}[h!tb]
  \centering
<<gens2, cache=FALSE,echo=FALSE,fig.height=4>>=
ggplot(data.generations,aes(x=gap,y=evaluations/time,group=strategy, color=strategy))+geom_point()+ stat_summary(fun.y="mean", geom="line", size=2)+scale_y_log10()+scale_x_log10(name="Generations",breaks=c(4,8,16,32,64))+theme_tufte()+labs(x="Generations",y="Evaluations/second")
@ 
\caption{Number of evaluations per second vs. generation gap for the baseline
  strategy (in pink) and the EDA and compress messaging strategies,
  with or without writeback (WB). Higher is better. Axes are logarithmic.}
\label{fig:evals2}
\end{figure}

Since we are exploring the parameter space, we will first try
to find out the generation gap and strategy that obtains the smaller
number of evaluations, indicating it is the best algorithmic
strategy. These are shown in Figure \ref{fig:evals}, where the two
messaging strategies, EDA and compress, with or without writeback, are
plotted and compared with the baseline strategy, which uses the full,
evaluated population as a message. The first observation is that, in
general, the number of evaluations increases with the generation
gap. More evolution without interchange with other populations implies
more exploitation, and then the possibility of stagnation. For 8
generations the results are very similar, but they diverge with the
generation gap. Clearly, the baseline strategy achieves the lowest
number of evaluations, for two reasons: it does not need to
re-evaluate the population when the message is received, but also, in
the case of EDA, the population is rebuilt from its statistical
description, so the exact individual (with the possible highest
fitness) is not kept. The lowest number of evaluations, although not
significantly so, is achieved for the EDA strategy with no writeback.

This chart can also be used to check the performance of the {\em
  no-writeback} strategies. In principle, since they use more evolved
populations, they should be better. As a matter of fact, since they
are injecting an additional population that is actually evolved, they
use more evaluations. So in principle these strategies will be
discarded.

Additionally, we evaluate the raw performance, in evaluations per
second, of all strategies, since at the end of the day, working with
concurrent evolutionary algorithms pursue higher speed. This is shown
in Figure \ref{fig:evals2}, which shows in the $y$ axis the number of
evaluations per second. In this case, the EDA strategy is clearly
superior to the rest, beating them significantly for all generation
gaps. In this case, generation gap == 8 will be chosen, since it
achieves the best combination of algorithmic and wallclock
performance.

Our initial intention in this paper was to design a communication
strategy that improves the speed of the algorithm, and with the EDA
strategy we have achieved just that. However, there are two parts in
this strategy: using significantly smaller messages, and moving
selection strategy from the mixer to the evolver (when computing the
probability distribution). In fact, 1/4 of the population is used to
compute the distribution, as opposed to the baseline mixer (labeled
``Full''), which takes the better half of the pair of populations for
mixing. This is undoubtedly a factor that contributes to balance the
number of additional evaluations needed for the new, rebuilt,
populations. However, the increase of speed must be entirely due to
the compactness of the messages used by this strategy.

\begin{figure*}[h!tb]
  \centering
\includegraphics[width=0.95\textwidth]{../figure/screenshot}
\caption{Screenshot using the htop utility of the used machine running
  two experiments at the same time. As it can be seen, all processors
  are kept busy, with a very high load average. }
\label{fig:screenshot}
\end{figure*}
%
However, the intention of concurrent evolutionary algorithms is to
leverage the power of all threads and processors in a computer, so
unlike in previous papers, we must find a version of the algorithm
that speeds up with the number of threads. We will settle on the
communication strategy, EDA, that has been proved to work before, and
the population size (256) and generation gap (8) that has been proved
to work previously. The number of initial populations was set to the
number of threads plus one, as the minimum required to avoid
starvation. 
However, we need to devise a strategy that
actually makes scaling possible and profitable.
After many tests, eventually the scaling strategy was simply to divide
the total population by the number of threads. Initially we had two
threads and total population equal to 512, so our strategy, called AP
for adaptive population, was to divide the number of total individuals
between the threads, so that 4 threads, for instance, get 128
individuals each. We repeated every run 15 times.

In this case, experiments were run on a different machine with the
Ubuntu 18.04 OS and an AMD Ryzen 7 2700X Eight-Core Processor
at 3.7GHz. Figure \ref{fig:screenshot} shows the utility htop with an
experiment running; the top of the screen shows the rate at which all
cores are working, showing all of them occupied; of course, the
program was not running exclusively, but the list of processes below
show how the program is replicated in several processor, thus
leveraging their full power. Please check also the number of threads
that are actually running at the same time, a few of which are being used by our application. 

\begin{figure}[h!tb]
  \centering
<<threads1, cache=FALSE,echo=FALSE,fig.height=4,message=FALSE>>=
data.freqs.ap.threads <- read.csv('../data/gecco-2019-freqs-ap-threads.csv')
ggplot(data.freqs.ap.threads,aes(x=Threads,y=Evaluations,group=Threads)) + geom_boxplot(fill='green',notch=TRUE)+theme_tufte()+scale_x_continuous(name="Threads",breaks=c(2,4,6,8))+labs(x="Threads",y="Evaluations",title="Scaling")
@ 
\caption{Number of evaluations vs. number of threads. Higher is better.}
\label{fig:threads1}
\end{figure}
%
\begin{figure}[h!tb]
  \centering
<<threads2, cache=FALSE,echo=FALSE,fig.height=4,message=FALSE>>=
ggplot(data.freqs.ap.threads,aes(x=Threads,y=Time,group=Threads))+geom_boxplot(fill='blue',notch=TRUE)+theme_tufte()+scale_x_continuous(name="Threads",breaks=c(2,4,6,8))+labs(x="Generation.Gap",y="Time",title="Scaling")
@ 
\caption{Total time vs. number of threads. Lower is better.}
\label{fig:threads2}
\end{figure}
%
We are first interested in the number of evaluations needed to find
the solution, which are plotted in Fig. \ref{fig:threads1}. It
increases slightly and not significantly from 2 to 4 threads, but it
does increase significantly for 6 and 8 threads, indicating that the
algorithm is performing significantly worse when we increase the number
of threads. This is probably due to the fact that we are
simultaneously decreasing the population size, leading to earlier
convergence for the number of generations (8) it is being used. This
interplay between the degree of concurrency, the population size and
the number of generations will have to be explored further.

\begin{figure}[h!tb]
  \centering
<<threads3, cache=FALSE,echo=FALSE,fig.height=4,message=FALSE>>=
ggplot(data.freqs.ap.threads,aes(x=Threads,y=Evaluations/Time,group=Threads))+geom_boxplot(fill='yellow',notch=TRUE)+theme_tufte()+scale_x_continuous(name="Threads",breaks=c(2,4,6,8))+labs(x="Generation.Gap",y="Evaluation/time",title="Scaling")
@ 
\caption{Evaluations per second vs. number of threads. Higher is better.}
\label{fig:threads3}
\end{figure}
%
But we were also interested in the actual wallclock time, plotted in
Fig. \ref{fig:threads2}. The picture shows that it decreases
significantly when we go from 2 (the baseline) to 4 threads, since we
are using more computing power for (roughly) the same number of
evaluations. It then increases slightly we we increase the number of
threads; as a matter of fact and as shown in \ref{fig:threads3}, the
number of evaluations per second increases steeply up to 6 threads,
and slightly when we use 8 threads. However, the amount of evaluations
needed overcompensates this speed, resulting in a worse results. It
confirms, however, that we are actually using all threads for
evaluations, and if only we could find a strategy that didn't need
more evaluations we should be able to get a big boost in computation
time that scales gracefully to a high number of processors.



\section{Conclusions}
\label{sec:conclusions}

Designing a concurrent, stateless evolutionary algorithm brings a new
set of configuration decisions that must be taken at the algorithm and
at the parameter level. Thus, the main intention in this paper was to
explore the parameter space in a concurrent evolutionary algorithm
looking for the combination that yields the best performance in terms
of time, without sacrificing the algorithmic performance. Since the
biggest obstacle to scaling and high performance was the size of
messages interchanged with the channel, in this paper we decided to
redesign this communication either in an algorithmic specific way,
using the distribution of probabilities as a representation of the
population, or in a data structure specific way, compressing the
bitstring to actual bits in a binary message. We also tested different messaging strategies.

Experiments show that no matter what the communication or messaging strategy is, we need to keep the number of generations the population undergoes to a small number, which resulted to be 8 in this case. This is equivalent to 2048 evaluations, and this is a number which is probably dependent on the problem and the data structure we are evolving. We would need to investigate this number further, and find a methodology to set it in advance, avoiding heuristics. That is left as a future line of work, but meanwhile our conclusion would be the importance of this generation gap in stateless evolutionary algorithms and the need to follow an experimental strategy to establish its value for particular problems or implementations. 

These experiments also yielded the EDA strategy as the fastest, with a relatively low impact in the number of evaluations. The fact that messages are the most compact is probably the main reason, but the fact that it uses a higher selective pressure is probably also a factor that should be taken into account.

Finally, this paper has proved for the first time the good scaling
behavior of this kind of concurrent evolutionary
algorithms. Simultaneous threads running an evolutionary algorithm do
increase the number of simultaneous evaluations, although since this
scaling is achieved via population splitting, the actual time achieved
reaches its lowest peak with just 4 threads. As a matter of fact, and
probably due to hardware limitations, the number of simultaneous
evaluations seems to reach a plateau with 8 threads. At any rate, this
new concurrent evolutionary algorithm is achieving results that are
much better than what the equivalent, single thread, evolutionary
algorithm would achieve. It should be noted that the base algorithm
used for comparison uses 3 threads already: two evolutionary threads
and a mixing thread; thus, the scaled-up version would use a total of
9 threads (8 evolutionary + 1 for mixing).

Although the good performance of this concurrent evolutionary
algorithm has been well established in this paper, several lines of
research are open. The first one is to continue exploring the
algorithm itself trying to find out a scaling strategy that does not
have a negative influence in the number of evaluations, so that actual
increasing in the number of simultaneous evaluations achieved the
equivalent speedup. The second like would be to find out what is the
maximum number of simultaneous evaluations that can be achieved, so
that an optimum number of threads can be advised, if possible
independently of the problem and depending only on the physical number
of threads available.

The messaging strategies proposed here can be used only if the problem
can be represented via a binary data structure. New strategies will
have to be explored for floating-point representation, or more
complicated data structures. There are general-purpose strategies for
compressing messages, for instance, and they could be used in this
case. Estimation of distribution algorithms can also be extended to
other types of data, so this is is something that can be also explored.

Finally, we are using the same kind of algorithm in all
threads. Nothing prevents us from using a different algorithm per
thread, or using different parameters per thread. This opens a vast
space of possibilities, but the payoff might be worth it.



\begin{acks}
Acknowledgements taking\\
this much\\
space

\end{acks}

\bibliographystyle{ACM-Reference-Format}
\bibliography{../geneura,../concurrent,../perl6} 

\end{document}
