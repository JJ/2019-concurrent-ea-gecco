%\documentclass[sigconf, authordraft]{acmart}
\documentclass[sigconf]{acmart}

\usepackage{booktabs} % For formal tables
\usepackage{tikz}
\usepackage{pgfbaselayers}
\usepackage[underline=false]{pgf-umlsd}
\usetikzlibrary{shadows}
\usetikzlibrary{arrows}

% Copyright
%\setcopyright{none}
%\setcopyright{acmcopyright}
%\setcopyright{acmlicensed}
\setcopyright{rightsretained}
%\setcopyright{usgov}
%\setcopyright{usgovmixed}
%\setcopyright{cagov}
%\setcopyright{cagovmixed}


% DOI
\acmDOI{10.1145/nnnnnnn.nnnnnnn}

% ISBN
\acmISBN{978-x-xxxx-xxxx-x/YY/MM}

% Conference
\acmConference[GECCO '19]{the Genetic and Evolutionary Computation Conference 2019}{July 13--17, 2019}{Prague, Czech Republic}
\acmYear{2019}
\copyrightyear{2019}

%\acmArticle{4}
\acmPrice{15.00}

% These commands are optional
%\acmBooktitle{Transactions of the ACM Woodstock conference}
%\editor{Jennifer B. Sartor}
%\editor{Theo D'Hondt}
%\editor{Wolfgang De Meuter}

\hypersetup{draft} 
\begin{document}

<<setup, cache=FALSE,echo=FALSE>>=
library(ggplot2)
library(ggthemes)

data.freqs.nw.gens <- read.csv('../data/gecco-2019-freqs-noweb-generations.csv')
data.compress.nw.gens <- read.csv('../data/gecco-2019-compress-noweb-generations.csv')
data.freqs.generations <- read.csv('../data/gecco-2019-freqs-generations.csv')
data.compress.generations <- read.csv('../data/gecco-2019-compress-generations.csv')
data.evostar <- read.csv("../data/evostar2019.csv")

data.generations <- data.frame(
    strategy=c(rep("Compress",length(data.compress.generations$Generation.Gap)),
               rep("CompressNW",length(data.compress.nw.gens$Generation.Gap)),
               rep("EDA",length(data.freqs.generations$Generation.Gap)),
               rep("EDANW",length(data.freqs.nw.gens$Generation.Gap)),
               rep("Full",length(data.evostar$gap))),
    gap=c(data.compress.generations$Generation.Gap,data.compress.nw.gens$Generation.Gap,data.freqs.generations$Generation.Gap,data.freqs.nw.gens$Generation.Gap,data.evostar$gap),
    evaluations=c(data.compress.generations$Evaluations,data.compress.nw.gens$Evaluations,data.freqs.generations$Evaluations,data.freqs.nw.gens$Evaluation,data.evostar$evaluations),
    time=c(data.compress.generations$Time,data.compress.nw.gens$Time,data.freqs.generations$Time,data.freqs.nw.gens$Time,data.evostar$time) )

@

%----------------------------------------------------------------
\title{Improving the algorithmic efficiency and performance of channel-based evolutionary algorithms}
% Same title as EvoApp paper? - Juanlu
% I hadn't realized that... 
% I propose that the subtitle becomes the title and the subtitle becomes something like: "The case of a Perl6-based implementation" -  Juanlu

  \author{Juan-Juli\'an Merelo Guerv\'os}
%  \orcid{1234-5678-9012}
  \affiliation{%
    \institution{Universidad de Granada/CITIC}
    \city{Granada,Spain}
    \postcode{18071}
  }
  \email{jjmerelo@gmail.com}
  
  \author{Juan Luis Jim\'enez Laredo}
  \affiliation{%
    \institution{Ri2C-LITIS, Universit\'e Le Havre}
    \city{Le Havre, France}
  }
  \email{juanlu.jimenez@univ-lehavre.fr}

  \author{Pedro A. Castillo}
  \affiliation{%
    \institution{Universidad de Granada/CITIC}
    \city{Granada,Spain}
    \postcode{18071}
  }
  \email{pacv@ugr.es}

  \author{Mario Garc\'ia Valdez}
  \affiliation{%
    \institution{Tecnol\'ogico Nacional de M\'exico}
    \city{Tijuana}
    \state{M\'exico}
    \postcode{22414}
  }
  \email{mario@tectijuana.edu.mx}

  \author{Sergio Rojas-Galeano}
  \affiliation{%
    \institution{{\small Universidad Distrital Francisco Jos\'e de Caldas}}
    \city{Bogot\'a}
    \state{Colombia}
    \postcode{111311}
  }
  \email{srojas@udistrital.edu.co}
  
  % The default list of authors is too long for headers.
  \renewcommand{\shortauthors}{A. Author et al.}

  
%----------------------------------------------------------------
\begin{abstract}
Concurrent evolutionary algorithms use threads that  communicate via
messages. Parametrizing how much work is done in every thread and the
way they communicate result is the main challenge in its design. In
this paper we work with concurrent evolutionary algorithms implemented
in Perl 6, and explore different options of single-thread evolution
parameters, communication and mixing of results, and eventually
scaling achieved in a multi-core environment.

\end{abstract}

%
% The code below should be generated by the tool at
% http://dl.acm.org/ccs.cfm
% Please copy and paste the code instead of the example below. 
%
 \begin{CCSXML}
<ccs2012>
<concept>
<concept_id>10003752.10003809.10011778</concept_id>
<concept_desc>Theory of computation~Concurrent algorithms</concept_desc>
<concept_significance>500</concept_significance>
</concept>
<concept>
<concept_id>10010147.10010257.10010293.10011809.10011812</concept_id>
<concept_desc>Computing methodologies~Genetic algorithms</concept_desc>
<concept_significance>500</concept_significance>
</concept>
</ccs2012>

\end{CCSXML}

\ccsdesc[500]{Theory of computation~Concurrent algorithms}
\ccsdesc[500]{Computing methodologies~Genetic algorithms}
\ccsdesc[300]{General and reference~Performance}

\keywords{Concurrent evolutionary algorithms, performance evaluation}


\maketitle

%----------------------------------------------------------------
\section{Introduction}

%% Despite the emphasis on algorithmic techniques whose foundation is on
%% software techniques that have an ultimate basis on newer hardware
%% features,
% The sentence is too large and somehow not clear enough. - Juanlu
% What about this one: 

%Despite the emphasis on leveraging newer hardware features
%with best-suited software techniques
%there are not many papers \cite{Xia2010} dealing with 
%creating concurrent evolutionary algorithms that work in a single
%computing node or that extend seamlessly from single to many
%computers. 
Nowadays, concurrent programming is the best option if we are dealing 
with a multi-core processor architecture where many processes 
and threads can coexists at the same time.
This eventually means that many processes (heavy or lightweight) 
can be leveraged to take full advantage of the processor capabilities.

These capabilities must be matched at an abstract level by concurrent languages 
that are characterized by the presence of programming constructs that manage processes.
The language provides an abstraction for communication using new communication constructs.
%that build on them so that high-level algorithms can use them without
%worrying about the low-level mechanisms of creation or destruction of
%threads, or how data is shared or communicated among them. These
%languages are called concurrent, and the programming paradigm
%implemented in them concurrency-oriented programming or simply
%concurrent programming \cite{Armstrong2003}. 
%These languages are characterized by the presence of
%programming constructs that manage processes like first class
%objects; that means that the language includes  operators for acting upon them and the
%possibility of using them like parameters or function's result
%values. This changes the coding of concurrent algorithms due to the
%direct mapping between patterns of communications and processes with
%language expressions; on the one hand it becomes simpler since the
%language provides an abstraction for communication, on the other hand
%it changes the paradigm for implementing algorithms, since these new
%communication constructs have to be taken into account. 
Moreover, concurrent programming adds a layer of abstraction over the parallel
facilities of processors and operating systems, offering a
high-level interface that allows the user to program modules of code to
be executed in parallel threads \cite{andrews1991concurrent}.
Different languages offer different concurrency strategies depending on how they deal with shared state,
that is, data structures that could be accessed from several processes. 

%In this regard, there are two major fields (with some other variations): 
%\begin{itemize}
%\item Actor-based concurrency \cite{schippers2009towards}
%totally eliminates shared state by introducing a series of data
%structures called {\em actors} that
%store state and can mutate it locally. 
%\item Process calculi or process algebra is a framework to describe
%  systems that work with independent
%  processes that interact between them using channels. One of the best
%  known is called the {\em communicating sequential processes} (CSP)
%methodology \cite{Hoare:1978:CSP:359576.359585}, which is effectively
%stateless, with different processes reacting to a channel input without
%changing state, and writing to these channels. Unlike actor based
%concurrency, which keeps state local, in this case per-process state is totally
%eliminated, with all computation state managed as messages in a channel.
%\item Other, less well known models using, for instance, tuple
%  spaces \cite{gelernter1985generative}.
%\end{itemize}

%Most modern languages, however, follow the CSP abstraction, and it has
%become popular since it fits well other programming paradigms, like
%reactive and functional programming, and allows for a more efficient
%implementation, with less overhead, and with well-defined
%primitives. This is why we will use it in this paper for creating new
%evolutionary algorithms that live {\em natively} in these environments
%and can thus be implemented easily in this kind of languages. We have chosen Perl 6, although 
%other languages such as Go, are feasible alternatives.

In previous papers \cite{Merelo:2018:MEA:3205651.3208317,merelo:WEA} we
designed an evolutionary algorithm based on using a stateless architecture, 
with different processes reacting to a channel input without changing state, and writing to the channel,
and explored its possibilities. 
%That initial exploration showed that a critical factor within 
%this algorithmic model is communication between threads; therefore designing
%efficient messages is high-priority to obtain a good algorithmic performance and scaling. 
In this paper, we will test several communication strategies: 
a lossless one that compresses the population, and a lossy one 
that sends a representation of population gene-wise statistics.

%----------------------------------------------------------------
\section{Experimental setup}
\label{sec:exp}

\begin{figure}[tb]
  \centering
\scalebox{.8}[.55]{
\begin{sequencediagram}

\newthread[red]{E}{Evolver} 

\tikzstyle{inststyle}+=[rounded corners=3mm] 
\newinst{C}{Channel}

\tikzstyle{inststyle}+=[rounded corners=0]
\newthread[blue]{M}{Mixer}

\begin{call}{E}{evolve()}{E}{}\end{call}

\setthreadbias{east}
\begin{messcall}{E}{$pop_1$}{C} 
\mess{C}{$pop_1$}{M}
\end{messcall}

\prelevel\prelevel
\begin{call}{E}{evolve()}{E}{}\end{call}

\setthreadbias{east}
\begin{messcall}{E}{$pop_2$}{C}  
\mess{C}{$pop_2$}{M}
\end{messcall}

\prelevel\prelevel
\begin{call}{M}{mix()}{M}{}\end{call}

\postlevel\postlevel
\setthreadbias{west}
\begin{messcall}{M}{\shortstack{ \{\ $mixpop_1$,\\ $mixpop_2$,\\ \vdots \\ $mixpop_k$ \} }}{C}

\mess{C}{$mixpop_1$}{E} 
\begin{call}{E}{evolve()}{E}{}\end{call}

\setthreadbias{east}
\begin{messcall}{E}{$pop_3$}{C} 
\postlevel
\mess{C}{$mixpop_2$}{E} 
%\prelevel
\mess{C}{$pop_3$}{M}
\end{messcall}

\prelevel\prelevel
\begin{call}{E}{evolve()}{E}{}\end{call}

\setthreadbias{east}
\begin{messcall}{E}{$pop_4$}{C}
\mess{C}{$pop_4$}{M}
\end{messcall}

\setthreadbias{west}
\prelevel
\mess{C}{$mixpop_k$}{E}%\end{messcall}

\end{messcall}

\prelevel\prelevel
\begin{call}{M}{mix()}{M}{\vdots}\end{call}
\prelevel
\begin{call}{E}{evolve()}{E}{\vdots}\end{call}

\end{sequencediagram}
}

\caption{Schematic of communication between threads and channels for
  concurrent EAs. 
%  The two central bars represent the channel, and
%  color corresponds to the {\em main} function they perform; blue for
%  mixer, red for evolver. As the figure shows, the evolver threads
%  always read from the mixer channel, and always write to the evolver channel.
  }
\label{fig:schematic}
\end{figure}


The baseline we are coming from is similar to the one used in previous experiments
\cite{Merelo:2018:MEA:3205651.3208317}. Our intention was to
create a system that was not functionally equivalent to a sequential
evolutionary algorithms, and that followed the principle of
communicating sequential processes. 
%In this kind of methodology, we
%will have processes (or threads) communicating state through
%channels. Every process itself will be stateless, reacting to the
%presence of messages in the channels it is listening to and sending
%result back to them, without changing state.
As in the previous papers, \cite{merelo:WEA}, we will use two
groups of threads and two channels:  %. We will see them in turns.

%The two groups of threads perform the following functions:

\begin{itemize}
\item The {\em evolutionary} threads will be the ones that will be in
  principle running the evolutionary algorithm.
\item The {\em mixing} threads will {\em mix} populations, and create
  new ones as a mixture of them.

%\end{itemize}
%
%The two channels carry messages that are equivalent to populations, but they do so in a different way:
%\begin{itemize}
  
\item The {\em evolutionary} channel will be used for carrying
  non-evolved, or generated, populations.
\item The {\em mixer} channel will carry, {\em in pairs}, evolved
  populations. 
\end{itemize}

The proposed system is described in detail in Figure \ref{fig:schematic}.
%These will be connected as shown in Figure \ref{fig:schematic}.
The evolutionary group of threads will read only from the evolutionary channel,
evolve for a number of generations, and place result in the mixer
channel; the mixer group of threads will read only from the mixer
channel, in pairs. From every pair, a random element is put back into
the mixer channel, and a new population is generated and sent back to
the evolutionary channel. 
The main objective of using two channels is
to avoid deadlocks; the fact that one population is written always
back to the mixer channel avoids starvation in the channel. 
%How this runs in practice is shown in Figure \ref{fig:schematic}, where the
%timeline of the interchange of messages between the evolver and mixer
%threads and evolver and mixer channels is clarified.

%One of the problems of the baseline configuration was that
%communication took a great amount of time, adding some overhead to the
%algorithm. The {\em message} consisted of the whole population, and
%the size increased with population size, obviously. This tipped the
%balance between communication and computation towards communication,
%so that the more threads, the more communication was taking place. Our
%first intention in this paper was to slim down messages so that they
%took less bandwidth (or memory) and less time to send and process. In
%general, this strategy also can be framed in the context of migration
%strategies, since that is the most similar thing in the context of
%parallel algorithms. In parallel algorithms, an adequate selection of
%migration strategies, balancing exploration and exploitation, is the
%key to achieving high performance, as indicated in
%\cite{Cantu-Paz:1999:MPT:2933923.2934003}.
%In the context of concurrent evolutionary algorithms we will talk
%about {\em population messages}, but their effect is going to be similar. 

In the baseline configuration the communication took a great amount of time. 
The {\em message} consisted of the whole population and the size increased with population size.
This strategy also can be framed in the context of migration strategies, since that is the most similar thing in the context of parallel algorithms.

For this paper, we introduced two different messaging strategies (i.e. migration strategies):  
{\em EDA}, in which the population message will contain the probability distribution over each gene,
and {\em compress}, that simply bit-packs the population without the fitness into a message, using 1 bit per individual.

%\begin{itemize}
%\item One we have called {\em EDA}, or estimation of distribution
%  algorithm, whose basic idea is that the population message will
%  contain the probability distribution over each gene. In this sense,
%  this strategy is similar to the one presented by de la Ossa et
%  al. in \cite{10.1007/978-3-540-30217-9_25}. Not being an estimation
%  of distribution algorithm {\em per se}, since the evolutionary
%  thread runs a canonical genetic algorithm, when the message is being
%  composed, every (binary) gene of the 25\% best individuals in the
%  population is examined, and an array with the
%  probabilities for each gene is sent to the mixer thread. The {\em
%    mixer} thread, in turn, just takes randomly one probability from
%  each of the two {\em populations} (actually, distributions), instead
%  of working on individuals. While in the baseline strategy the
%  selection took place in the mixer thread, that eliminated half the
%  population, in this case the selection takes place when composing
%  the message, since just the 25\% best individuals are selected to
%  compute the probability distribution of genes. When the evolver
%  thread reads the message, it rebuilds the population based on this
%  distribution.
%\item The second is called {\em compress}, and it simply bit-packs the
%  population, without the fitness, into a message which uses 1 bit per
%  individual, and then 64 bits, or simply 8 bytes, to transmit a
%  single individual in the population. This strategy is equivalent to
%  the baseline, except it introduces an additional step of evaluating
%  the population when mixing and receiving it from the evolutionary
%  channel. It is hoped that this additional evaluation overhead does
%  compensates the communication overhead that is eliminated.
%\end{itemize}


%In the same way we did in our previous papers, first we will have to
%evaluate these new strategies compared with baseline; since the
%overhead will be different depending on the computation time, which in
%this case is regulated by the number of generations that are going to
%be performed by every thread, we will first perform an experiment
%changing that. We will call this parameter the {\em generation gap},
%implying that's the gap between receiving a message and activating the
%thread and sending it, deactivating it. 
%Besides, what we want to find out in these set of experiments is what is the 
%generation gap that gives the best performance in terms of raw time to
%find a solution, as well as the best number of evaluations per second.

In the same way we did in our previous papers, first we will compare 
these new strategies compared with baseline, 
evaluating the gap between receiving a message and activating the thread and sending it, deactivating it.
Additionally, We will heuristically explore other messaging
strategies called {\em no writeback} ({\sf nw}), where the mixer
thread sends the individuals to the evolver channel, where it will undergo
an additional round of evolution.

%Additionally, it is impossible to know from first principles if this
%setup is the only possible. We have to heuristically explore other
%possibilities; in this case, we will explore another messaging
%strategy called {\em no writeback}, or {\sf nw}, where the mixer
%thread, instead of sending one of the individuals back again to the
%mixer thread, sends it to the evolver channel, where it will undergo
%an additional round of evolution. The main difference between these
%two strategies is twofold:\begin{itemize}
%\item The mixer channel can be empty for some time, since it is not
%  always holding at least one population message (written back every
%  time it is activated). This might lead to {\em starvation} of the
%  mixer thread, but in fact it will not take long since it is going to
%  be processed immediately by the evolver thread.
%\item Every population is mixed just once, which might lead to
%  improvements in the algorithm.
%\end{itemize}


%----------------------------------------------------------------
\section{Experimental results}
\label{sec:exp}

The experiments have been prepared by using OneMax function with 64
bits. This function was chosen primarily since it was the one used in
previous experiments, but also because it is a classical benchmark, it
can be easily programmed in Perl 6, and allowed us to focus in what we
are interested in, the design of the concurrent evolutionary algorithm
itself. We used the open source {\tt Algorithm::Evolutionary::Simple}
Perl 6 module. % for the evolutionary part, and wrote the different
%scripts in the same language. The latest version, compiled from
%source, of Perl 6 was used, and experiments were performed in a
%Intel(R) Core(TM) i7-4770 CPU at 3.40GHz running Ubuntu 14.04 server.
%All scripts have a free license and have been released in GitHub,
%where the data generated by every single experiment is also hosted.

\begin{table}
  \centering
  \caption{Parameters used to explore the generation gap}
  \label{tab:gens}
  \begin{tabular}{|lr|lr|}
    \toprule
    Parameter & Value & Parameter & Value\\
    \midrule
Evolver threads & 2    &  Population size & 256 \\
Mixer threads & 1      &  Initial populations & 3 \\
Generations & 8,16,32  &  Bits & 64   \\
Repetitions & $>=$ 15  &       &      \\
  \bottomrule
  \end{tabular}
\end{table}

The parameters used are shown in Table
\ref{tab:gens}. Two evolver threads are needed, at least, to avoid
starvation of the mixer thread. The generation gap was checked for
those values in all cases, although in some cases we extended it to 4
and 64 generations. The population was sized in previous papers using
the bisection method, and the number of initial populations created
and sent to the evolver channel was also designed to avoid starvation.
%that way, as soon as the first two populations are evaluated
%simultaneously by the two threads and sent to the mixer channel, this
%channel will always hold a population that will combine with a fresh
%one coming from either of the mixer threads.

First, the generation gap and strategy that obtains the smaller
number of evaluations has been found.
%As shown in Figure \ref{fig:threads2}, 
In general, the number of evaluations increases with the generation
gap. More evolution without interchange with other populations implies
more exploitation, and then the possibility of stagnation.
The lowest number of evaluations is achieved for the EDA strategy with no writeback.

%Since we are exploring the parameter space, we will first try
%to find out the generation gap and strategy that obtains the smaller
%number of evaluations, indicating it is the best algorithmic
%strategy. These are shown in Figure \ref{fig:evals}, where the two
%messaging strategies, EDA and compress, with or without writeback, are
%plotted and compared with the baseline strategy, which uses the full,
%evaluated population as a message. The first observation is that, in
%general, the number of evaluations increases with the generation
%gap. More evolution without interchange with other populations implies
%more exploitation, and then the possibility of stagnation. For 8
%generations the results are very similar, but they diverge with the
%generation gap. Clearly, the baseline strategy achieves the lowest
%number of evaluations, for two reasons: it does not need to
%re-evaluate the population when the message is received, but also, in
%the case of EDA, the population is rebuilt from its statistical
%description, so the exact individual (with the possible highest
%fitness) is not kept. The lowest number of evaluations, although not
%significantly so, is achieved for the EDA strategy with no writeback.

%This chart can also be used to check the performance of the {\em no-writeback} 
%strategies. In principle, since they use more evolved
%populations, they should be better. As a matter of fact, since they
%are injecting an additional population that is actually evolved, they
%use more evaluations. So in principle these strategies will be
%discarded.

Additionally, the raw performance in evaluations per second of all strategies has been evaluated, 
showing that the EDA strategy is superior to the rest for all generation gaps. 

%Additionally, we evaluate the raw performance, in evaluations per
%second, of all strategies, since at the end of the day, working with
%concurrent evolutionary algorithms pursue higher speed. This is shown
%in Figure \ref{fig:evals2}, which shows in the $y$ axis the number of
%evaluations per second. In this case, the EDA strategy is clearly
%superior to the rest, beating them significantly for all generation
%gaps. In this case, generation gap == 8 will be chosen, since it
%achieves the best combination of algorithmic and wallclock
%performance.

As a result, using EDA strategy, a communication strategy that improves the speed of the algorithm has been obtained, mainly due to the compactness of the messages used by this strategy.

%Our initial intention in this paper was to design a communication
%strategy that improves the speed of the algorithm, and with the EDA
%strategy we have achieved just that. However, there are two parts in
%this strategy: using significantly smaller messages, and moving
%selection strategy from the mixer to the evolver (when computing the
%probability distribution). In fact, 1/4 of the population is used to
%compute the distribution, as opposed to the baseline mixer (labeled
%``Full''), which takes the better half of the pair of populations for
%mixing. This is undoubtedly a factor that contributes to balance the
%number of additional evaluations needed for the new, rebuilt,
%populations. However, the increase of speed must be entirely due to
%the compactness of the messages used by this strategy.

%However, the intention of concurrent evolutionary algorithms is to
%leverage the power of all threads and processors in a computer, so
%unlike in previous papers, we must find a version of the algorithm
%that speeds up with the number of threads.
%After many tests, eventually the scaling strategy was simply to divide
%the total population by the number of threads.

%However, the intention of concurrent evolutionary algorithms is to
%leverage the power of all threads and processors in a computer, so
%unlike in previous papers, we must find a version of the algorithm
%that speeds up with the number of threads. We will settle on the
%communication strategy, EDA, that has been proved to work before, and
%the population size (256) and generation gap (8) that has been proved
%to work previously. The number of initial populations was set to the
%number of threads plus one, as the minimum required to avoid starvation. 
%However, we need to devise a strategy that actually makes scaling possible and profitable.
%After many tests, eventually the scaling strategy was simply to divide
%the total population by the number of threads. Initially we had two
%threads and total population equal to 512, so our strategy, called AP
%for adaptive population, was to divide the number of total individuals
%between the threads, so that 4 threads, for instance, get 128
%individuals each. We repeated every run 15 times.

%In this case, experiments were run on a different machine with the
%Ubuntu 18.04 OS and an AMD Ryzen 7 2700X Eight-Core Processor
%at 3.7GHz. Figure \ref{fig:screenshot} shows the utility htop with an
%experiment running; the top of the screen shows the rate at which all
%cores are working, showing all of them occupied; of course, the
%program was not running exclusively, but the list of processes below
%show how the program is replicated in several processor, thus
%leveraging their full power. Please check also the number of threads
%that are actually running at the same time, a few of which are being used by our application. 

%\begin{figure}[h!tb]
%  \centering
%<<threads2, cache=FALSE,echo=FALSE,fig.height=4,message=FALSE>>=
%ggplot(data.freqs.ap.threads,aes(x=Threads,y=Time,group=Threads))+geom_boxplot(fill='blue',notch=TRUE)+theme_tufte()+scale_x_continuous(name="Threads",breaks=c(2,4,6,8))+labs(x="Generation.Gap",y="Time",title="Scaling")
%@ 
%\caption{Total time vs. number of threads. Lower is better.}
%\label{fig:threads2}
%\end{figure}

%We are first interested in the number of evaluations needed to find
%the solution, which are plotted in Fig. \ref{fig:threads1}. It
%increases slightly and not significantly from 2 to 4 threads, but it
%does increase significantly for 6 and 8 threads, indicating that the
%algorithm is performing significantly worse when we increase the number
%of threads. This is probably due to the fact that we are
%simultaneously decreasing the population size, leading to earlier
%convergence for the number of generations (8) it is being used. This
%interplay between the degree of concurrency, the population size and
%the number of generations will have to be explored further.

%But we were also interested in the actual wallclock time, plotted in
%Fig. \ref{fig:threads2}. The picture shows that it decreases
%significantly when we go from 2 (the baseline) to 4 threads, since we
%are using more computing power for (roughly) the same number of
%evaluations. It then increases slightly we we increase the number of
%threads; as a matter of fact and as shown in \ref{fig:threads3}, the
%number of evaluations per second increases steeply up to 6 threads,
%and slightly when we use 8 threads. However, the amount of evaluations
%needed overcompensates this speed, resulting in a worse results. It
%confirms, however, that we are actually using all threads for
%evaluations, and if only we could find a strategy that didn't need
%more evaluations we should be able to get a big boost in computation
%time that scales gracefully to a high number of processors.


%----------------------------------------------------------------
\section{Conclusions}
\label{sec:conclusions}

In this paper we explore the parameter space in a concurrent evolutionary algorithm
looking for the combination that yields the best performance in terms
of time, without sacrificing the algorithmic performance.
In order to do so, the size of messages interchanged within the channel
has been redesigned using the distribution of probabilities as a representation of the
population.
Different messaging strategies has been also tested.

%Designing a concurrent, stateless evolutionary algorithm brings a new
%set of configuration decisions that must be taken into account at the algorithm and
%at the parameter level. Thus, the main intention in this paper was to
%explore the parameter space in a concurrent evolutionary algorithm
%looking for the combination that yields the best performance in terms
%of time, without sacrificing the algorithmic performance. Since the
%biggest obstacle to scaling and high performance was the size of
%messages interchanged within the channel, in this paper we decided to
%redesign this communication either in an algorithmic specific way:
%using the distribution of probabilities as a representation of the
%population; or in a data structure specific way: compressing the
%bitstring to actual bits in a binary message. 
%We also tested different messaging strategies.

Experiments show that the number of generations that the population undergoes must be keep to a small number.
Experiments also yielded the EDA strategy as the fastest, with a relatively low impact in the number of evaluations, as the messages are the most compact.
Finally, obtained results have shown that simultaneous threads running 
an evolutionary algorithm via population splitting do increase the number of simultaneous evaluations,
leading the new concurrent evolutionary algorithm to achieve results that are
much better than those that the equivalent, single thread evolutionary algorithm would achieve.

%Experiments show that no matter what the communication or messaging strategy is, we need to keep the number of generations that the population undergoes to a small number, which resulted to be 8 in this case. This is equivalent to 2048 evaluations: a number which is likely dependent on the problem and the data structure we are evolving. 
%We would need to investigate this number further, and find a methodology to set it in advance, avoiding heuristics. That is left as a future line of work, but meanwhile our conclusion would be the importance of this generation gap in stateless evolutionary algorithms and the need to follow an experimental strategy to establish its value for particular problems or implementations. 

%These experiments also yielded the EDA strategy as the fastest, with a relatively low impact in the number of evaluations. The fact that messages are the most compact is probably the main reason, but the fact that it uses a higher selective pressure is probably also a factor that should be taken into account.

%Finally, this paper has shown for the first time the good scaling
%behavior of this kind of concurrent evolutionary
%algorithms. Simultaneous threads running an evolutionary algorithm do
%increase the number of simultaneous evaluations, although since this
%scaling is achieved via population splitting, the actual time achieved
%reaches its lowest peak with just 4 threads. As a matter of fact, and
%probably due to hardware limitations, the number of simultaneous
%evaluations seems to reach a plateau with 8 threads. At any rate, this
%new concurrent evolutionary algorithm is achieving results that are
%much better than what the equivalent, single thread, evolutionary
%algorithm would achieve. It should be noted that the base algorithm
%used for comparison uses 3 threads already: two evolutionary threads
%and a mixing thread; thus, the scaled-up version would use a total of
%9 threads (8 evolutionary + 1 for mixing).


%----------------------------------------------------------------
\begin{acks}
This paper has been supported in part by
\href{http://geneura.wordpress.com}{GeNeura Team}, 
projects DeepBio (TIN2017-85727-C4-2-P) and TecNM Project 5654.19-P. 
\end{acks}


%----------------------------------------------------------------
\bibliographystyle{ACM-Reference-Format}
\bibliography{../geneura,../concurrent,../perl6} 

\end{document}
