%\documentclass[sigconf, authordraft]{acmart}
\documentclass[sigconf]{acmart}

\usepackage{booktabs} % For formal tables
\usepackage{tikz}
\usepackage{pgfbaselayers}
\usepackage[underline=false]{pgf-umlsd}
\usepackage{wrapfig}
\usetikzlibrary{shadows}
\usetikzlibrary{arrows}

\copyrightyear{2019}
\acmYear{2019}
\setcopyright{acmlicensed}
\acmConference[GECCO '19 Companion]{Genetic and Evolutionary
Computation Conference Companion}{July 13--17, 2019}{Prague, Czech
Republic}
\acmBooktitle{Genetic and Evolutionary Computation Conference Companion
(GECCO '19 Companion), July 13--17, 2019, Prague, Czech Republic}
\acmPrice{15.00}
\acmDOI{10.1145/3319619.3322042}\acmISBN{978-1-4503-6748-6/19/07}

\hypersetup{draft} 
\begin{document}

<<setup, cache=FALSE,echo=FALSE>>=
library(ggplot2)
library(ggthemes)

data.freqs.nw.gens <- read.csv('../data/gecco-2019-freqs-noweb-generations.csv')
data.compress.nw.gens <- read.csv('../data/gecco-2019-compress-noweb-generations.csv')
data.freqs.generations <- read.csv('../data/gecco-2019-freqs-generations.csv')
data.compress.generations <- read.csv('../data/gecco-2019-compress-generations.csv')
data.evostar <- read.csv("../data/evostar2019.csv")

data.generations <- data.frame(
    strategy=c(rep("Compress",length(data.compress.generations$Generation.Gap)),
               rep("CompressNW",length(data.compress.nw.gens$Generation.Gap)),
               rep("EDA",length(data.freqs.generations$Generation.Gap)),
               rep("EDANW",length(data.freqs.nw.gens$Generation.Gap)),
               rep("Full",length(data.evostar$gap))),
    gap=c(data.compress.generations$Generation.Gap,data.compress.nw.gens$Generation.Gap,data.freqs.generations$Generation.Gap,data.freqs.nw.gens$Generation.Gap,data.evostar$gap),
    evaluations=c(data.compress.generations$Evaluations,data.compress.nw.gens$Evaluations,data.freqs.generations$Evaluations,data.freqs.nw.gens$Evaluation,data.evostar$evaluations),
    time=c(data.compress.generations$Time,data.compress.nw.gens$Time,data.freqs.generations$Time,data.freqs.nw.gens$Time,data.evostar$time) )

@

%----------------------------------------------------------------
\title{Improving the algorithmic efficiency and performance of channel-based evolutionary algorithms}
% Same title as EvoApp paper? - Juanlu
% I hadn't realized that... 
% I propose that the subtitle becomes the title and the subtitle becomes something like: "The case of a Perl6-based implementation" -  Juanlu

  \author{Juan-Juli\'an Merelo Guerv\'os}
%  \orcid{1234-5678-9012}
  \affiliation{%
    \institution{Universidad de Granada/CITIC}
    \city{Granada,Spain}
    \postcode{18071}
  }
  \email{jjmerelo@gmail.com}
  
  \author{Juan Luis Jim\'enez Laredo}
  \affiliation{%
    \institution{Ri2C-LITIS, Universit\'e Le Havre}
    \city{Le Havre, France}
  }
  \email{juanlu.jimenez@univ-lehavre.fr}

  \author{Pedro A. Castillo}
  \affiliation{%
    \institution{Universidad de Granada/CITIC}
    \city{Granada,Spain}
    \postcode{18071}
  }
  \email{pacv@ugr.es}

  \author{Mario Garc\'ia Valdez}
  \affiliation{%
    \institution{Tecnol\'ogico Nacional de M\'exico}
    \city{Tijuana}
    \state{M\'exico}
    \postcode{22414}
  }
  \email{mario@tectijuana.edu.mx}

  \author{Sergio Rojas-Galeano}
  \affiliation{%
    \institution{{\small Universidad Distrital Francisco Jos\'e de Caldas}}
    \city{Bogot\'a}
    \state{Colombia}
    \postcode{111311}
  }
  \email{srojas@udistrital.edu.co}
  
  % The default list of authors is too long for headers.
  \renewcommand{\shortauthors}{J.J. Merelo Guerv\'os et al.}

  
%----------------------------------------------------------------
\begin{abstract}
Concurrent evolutionary algorithms use threads that communicate via
messages. Parametrizing the work in every thread and the
way they communicate results is a major challenge in its design. In
this paper we work with concurrent evolutionary algorithms implemented
in Perl 6, and explore different options of single-thread evolution
parametrization, communication and mixing of results, showing that 
scalability is achieved in a multi-core environment.

\end{abstract}

%
% The code below should be generated by the tool at
% http://dl.acm.org/ccs.cfm
% Please copy and paste the code instead of the example below. 
%
 \begin{CCSXML}
<ccs2012>
<concept>
<concept_id>10003752.10003809.10011778</concept_id>
<concept_desc>Theory of computation~Concurrent algorithms</concept_desc>
<concept_significance>500</concept_significance>
</concept>
<concept>
<concept_id>10010147.10010257.10010293.10011809.10011812</concept_id>
<concept_desc>Computing methodologies~Genetic algorithms</concept_desc>
<concept_significance>500</concept_significance>
</concept>
</ccs2012>

\end{CCSXML}

\ccsdesc[500]{Theory of computation~Concurrent algorithms}
\ccsdesc[500]{Computing methodologies~Genetic algorithms}

\keywords{Concurrent evolutionary algorithms, performance evaluation}


\maketitle

%----------------------------------------------------------------
\section{Introduction}

Nowadays, concurrent programming is the best option to leverage the number of processes and threads that a multi-core processor architecture can host.
%
%% where many processes 
%% and threads can coexists at the same time.
%% This eventually means that many processes (heavy or lightweight) 
%% can be leveraged to take full advantage of the processor capabilities.
%
These capabilities must be matched at an abstract level by concurrent languages
that incorporate programming constructs intended to manage creation, execution 
and termination of processes, as well as new models of communication between such processes.
%These capabilities must be matched at an abstract level by concurrent languages 
%that are characterized by the presence of programming constructs that manage processes.
%The language provides an abstraction for communication using new communication constructs.
%that build on them so that high-level algorithms can use them without
%worrying about the low-level mechanisms of creation or destruction of
%threads, or how data is shared or communicated among them. These
%languages are called concurrent, and the programming paradigm
%implemented in them concurrency-oriented programming or simply
%concurrent programming \cite{Armstrong2003}. 
%These languages are characterized by the presence of
%programming constructs that manage processes like first class
%objects; that means that the language includes  operators for acting upon them and the
%possibility of using them like parameters or function's result
%values. This changes the coding of concurrent algorithms due to the
%direct mapping between patterns of communications and processes with
%language expressions; on the one hand it becomes simpler since the
%language provides an abstraction for communication, on the other hand
%it changes the paradigm for implementing algorithms, since these new
%communication constructs have to be taken into account. 
Moreover, concurrent programming adds a layer of abstraction over the parallel
facilities of processors and operating systems, offering a
high-level interface that allows the user to program modules of code to
be executed in parallel threads \cite{andrews1991concurrent}.
%
\begin{wrapfigure}[29]{r}{0.45\columnwidth}
  %\centering
  \vspace{-.5\intextsep}
\hspace*{-.95\columnsep}
\scalebox{.8}[.6]{
\begin{sequencediagram}

\newthread[red]{E}{Evolver} 

\tikzstyle{inststyle}+=[rounded corners=3mm] 
\newinst{C}{Channel}

\tikzstyle{inststyle}+=[rounded corners=0]
\newthread[blue]{M}{Mixer}

\begin{call}{E}{evolve()}{E}{}\end{call}

\setthreadbias{east}
\begin{messcall}{E}{$pop_1$}{C} 
\mess{C}{$pop_1$}{M}
\end{messcall}

\prelevel\prelevel
\begin{call}{E}{evolve()}{E}{}\end{call}

\setthreadbias{east}
\begin{messcall}{E}{$pop_2$}{C}  
\mess{C}{$pop_2$}{M}
\end{messcall}

\prelevel\prelevel
\begin{call}{M}{mix()}{M}{}\end{call}

\postlevel\postlevel
\setthreadbias{west}
\begin{messcall}{M}{\shortstack{ \{\ $mixpop_1$,\\ $mixpop_2$,\\ \vdots \\ $mixpop_k$ \} }}{C}

\mess{C}{$mixpop_1$}{E} 
\begin{call}{E}{evolve()}{E}{}\end{call}

\setthreadbias{east}
\begin{messcall}{E}{$pop_3$}{C} 
\postlevel
\mess{C}{$mixpop_2$}{E} 
%\prelevel
\mess{C}{$pop_3$}{M}
\end{messcall}

\prelevel\prelevel
\begin{call}{E}{evolve()}{E}{}\end{call}

\setthreadbias{east}
\begin{messcall}{E}{$pop_4$}{C}
\mess{C}{$pop_4$}{M}
\end{messcall}

\setthreadbias{west}
\prelevel
\mess{C}{$mixpop_k$}{E}%\end{messcall}

\end{messcall}

\prelevel\prelevel
\begin{call}{M}{mix()}{M}{\vdots}\end{call}
\prelevel
\begin{call}{E}{evolve()}{E}{\vdots}\end{call}
%\vspace{-.2cm}
\end{sequencediagram}
}
\caption{Concurrent EAs timeline }
\label{fig:schematic}
\end{wrapfigure}
%
Different languages offer different concurrency policies depending on how they deal with state, 
that is, data structures that could be accessed from several processes. They can be divided roughly between channel-based concurrency, with no shared nor stored state, and actor based concurrency, which stores state (in actors) but does not share it. This last model is the one used by the Perl 6 language, which is the one we are going to be using in this work.

Previously we
designed an evolutionary algorithm based on using a stateless architecture  \cite{Merelo:2018:MEA:3205651.3208317,merelo:WEA}, 
with different processes reacting to a channel input without changing
state, and writing to the channel, but its design leaves many options open, and they have to be explored heuristically. First, we will find out what are the best parameters from the point of view of the algorithm; then, we will test several communication strategies: 
a lossless one that compresses the population, and a lossy one 
that sends a representation of population gene-wise statistics.

%----------------------------------------------------------------
\section{Experimental setup and results}
\label{sec:exp}

Building upon the design we used in previous experiments\cite{Merelo:2018:MEA:3205651.3208317}, 
here our goal was to
%The baseline we are coming from is similar to the one used in previous experiments
%\cite{Merelo:2018:MEA:3205651.3208317}. Our intention was to
create a system that was not functionally equivalent to a sequential
evolutionary algorithms, and that followed the principle of
communicating sequential processes. 
%In this kind of methodology, we
%will have processes (or threads) communicating state through
%channels. Every process itself will be stateless, reacting to the
%presence of messages in the channels it is listening to and sending
%result back to them, without changing state.
As in the previous papers, \cite{merelo:WEA}, we will use two
groups of threads, one for performing the evolutionary algorithm and other for mixing populations, and two channels, one for carrying non-evolved, or generated, populations, and another for pairs of evolved
  populations. 

The proposed system is illustrated in Figure \ref{fig:schematic}.
%These will be connected as shown in Figure \ref{fig:schematic}.
The evolutionary group of threads will read only from the evolutionary channel,
evolve for a number of generations, and place result in the mixer
channel; the mixer group of threads will read only from the mixer
channel, in pairs. From every pair, a random element is put back into
the mixer channel, and a new population is obtained and sent back to
the evolutionary channel. 
The aim of using two channels is
to avoid deadlocks; the fact that one population is always written 
back to the mixer channel avoids starvation in that channel. 
%How this runs in practice is shown in Figure \ref{fig:schematic}, where the
%timeline of the interchange of messages between the evolver and mixer
%threads and evolver and mixer channels is clarified.

%One of the problems of the baseline configuration was that
%communication took a great amount of time, adding some overhead to the
%algorithm. The {\em message} consisted of the whole population, and
%the size increased with population size, obviously. This tipped the
%balance between communication and computation towards communication,
%so that the more threads, the more communication was taking place. Our
%first intention in this paper was to slim down messages so that they
%took less bandwidth (or memory) and less time to send and process. In
%general, this strategy also can be framed in the context of migration
%strategies, since that is the most similar thing in the context of
%parallel algorithms. In parallel algorithms, an adequate selection of
%migration strategies, balancing exploration and exploitation, is the
%key to achieving high performance, as indicated in
%\cite{Cantu-Paz:1999:MPT:2933923.2934003}.
%In the context of concurrent evolutionary algorithms we will talk
%about {\em population messages}, but their effect is going to be similar. 

The baseline communication model was time-costly so in this paper we
introduce two different messaging strategies:   
{\em EDA}, in which the message consist of a probability distribution of each gene in the population,
and {\em compress}, that simply bit-packs the population without the fitness into a message, using 1 bit per individual.

%\begin{itemize}
%\item One we have called {\em EDA}, or estimation of distribution
%  algorithm, whose basic idea is that the population message will
%  contain the probability distribution over each gene. In this sense,
%  this strategy is similar to the one presented by de la Ossa et
%  al. in \cite{10.1007/978-3-540-30217-9_25}. Not being an estimation
%  of distribution algorithm {\em per se}, since the evolutionary
%  thread runs a canonical genetic algorithm, when the message is being
%  composed, every (binary) gene of the 25\% best individuals in the
%  population is examined, and an array with the
%  probabilities for each gene is sent to the mixer thread. The {\em
%    mixer} thread, in turn, just takes randomly one probability from
%  each of the two {\em populations} (actually, distributions), instead
%  of working on individuals. While in the baseline strategy the
%  selection took place in the mixer thread, that eliminated half the
%  population, in this case the selection takes place when composing
%  the message, since just the 25\% best individuals are selected to
%  compute the probability distribution of genes. When the evolver
%  thread reads the message, it rebuilds the population based on this
%  distribution.
%\item The second is called {\em compress}, and it simply bit-packs the
%  population, without the fitness, into a message which uses 1 bit per
%  individual, and then 64 bits, or simply 8 bytes, to transmit a
%  single individual in the population. This strategy is equivalent to
%  the baseline, except it introduces an additional step of evaluating
%  the population when mixing and receiving it from the evolutionary
%  channel. It is hoped that this additional evaluation overhead does
%  compensates the communication overhead that is eliminated.
%\end{itemize}


%In the same way we did in our previous papers, first we will have to
%evaluate these new strategies compared with baseline; since the
%overhead will be different depending on the computation time, which in
%this case is regulated by the number of generations that are going to
%be performed by every thread, we will first perform an experiment
%changing that. We will call this parameter the {\em generation gap},
%implying that's the gap between receiving a message and activating the
%thread and sending it, deactivating it. 
%Besides, what we want to find out in these set of experiments is what is the 
%generation gap that gives the best performance in terms of raw time to
%find a solution, as well as the best number of evaluations per second.

Similarly to our previous experiments, first we will compare 
these new strategies with respect to a baseline, 
evaluating the gap between receiving a message and activating the thread until sending the message to deactivate it.
Besides, we heuristically studied other messaging
strategy called {\em no writeback} ({\sf nw}), where the mixer
thread sends the individuals to the evolver channel, to undergo
an additional round of evolution.

%Additionally, it is impossible to know from first principles if this
%setup is the only possible. We have to heuristically explore other
%possibilities; in this case, we will explore another messaging
%strategy called {\em no writeback}, or {\sf nw}, where the mixer
%thread, instead of sending one of the individuals back again to the
%mixer thread, sends it to the evolver channel, where it will undergo
%an additional round of evolution. The main difference between these
%two strategies is twofold:\begin{itemize}
%\item The mixer channel can be empty for some time, since it is not
%  always holding at least one population message (written back every
%  time it is activated). This might lead to {\em starvation} of the
%  mixer thread, but in fact it will not take long since it is going to
%  be processed immediately by the evolver thread.
%\item Every population is mixed just once, which might lead to
%  improvements in the algorithm.
%\end{itemize}


The experiments used 64-bit OneMax, a classical benchmark, it
can be easily programmed in Perl 6, and allowed us to focus in the 
design of the relevant mechanisms of the concurrent evolutionary
model; it was also used in the baseline experiments. 
We used the open source {\tt Algorithm::Evolutionary::Simple}
Perl 6 module. % for the evolutionary part, and wrote the different
%scripts in the same language. The latest version, compiled from
%source, of Perl 6 was used, and experiments were performed in a
%Intel(R) Core(TM) i7-4770 CPU at 3.40GHz running Ubuntu 14.04 server.
%All scripts have a free license and have been released in GitHub,
%where the data generated by every single experiment is also hosted.

%The parameters used are shown in Table
%\ref{tab:gens}.
Two evolver threads are needed to avoid
starvation of the mixer thread. The generation gap was checked for
the shown values, although in some cases we extended it to 4
and 64 generations. The population was sized as in previous papers using
the bisection method, and the number of initial populations created
and sent to the evolver channel was also designed to avoid starvation.
%that way, as soon as the first two populations are evaluated
%simultaneously by the two threads and sent to the mixer channel, this
%channel will always hold a population that will combine with a fresh
%one coming from either of the mixer threads.

%% \begin{table}[b!]
%%   \centering
%%   \caption{Parameters used to explore the generation gap}
%%   \footnotesize
%%   \label{tab:gens}
%%   \begin{tabular}{|lr|lr|}
%%     \hline
%%     Parameter & Value & Parameter & Value\\
%%     \hline
%% Evolver threads & 2    &  Population size & 256 \\
%% Mixer threads & 1      &  Initial populations & 3 \\
%% Generations & 8,16,32  &  Bits & 64   \\
%% Repetitions & $>=$ 15  &       &      \\
%%   \hline
%%   \end{tabular}
%% \end{table}
%% %
\begin{figure}[h!tb]
  \centering
<<gens2, cache=FALSE,echo=FALSE,fig.height=4>>=
ggplot(data.generations,aes(x=gap,y=evaluations/time,group=strategy, color=strategy))+geom_point()+ stat_summary(fun.y="mean", geom="line", size=2)+scale_y_log10()+scale_x_log10(name="Generations",breaks=c(4,8,16,32,64))+theme_tufte()+labs(x="Generations",y="Evaluations/second")
@ 
\caption{Evaluations per second vs. generation gap. Higher is better. Axes are logarithmic.}
\label{fig:evals2}
\end{figure}


First, the generation gap and strategy that obtains the best number of evaluations and evaluations per second has been found, and this is shown in Figure \ref{fig:evals2}
In general, the number of evaluations increases with the generation
gap. More evolution without interchange with other populations implies
more exploitation, and then the possibility of stagnation. However, the highest number of evaluations per second are achieved by the EDA strategies, as shown in the figure. This EDA-style communication strategy finds the best balance between number of evaluations and communication speed mainly due to the compactness of its messages, but also due to the fact that it is non-elitist and might avoid stagnation or dominance of the population by a super-individual.

However, the intention of concurrent evolutionary algorithms is to
leverage the power of all threads and processors in a computer, so
unlike in previous papers, we must find a version of the algorithm
that speeds up with the number of threads.
After many tests, eventually the scaling strategy was simply to divide
the total population by the number of threads. This resulted in a
decrease of wallclock time from 2 to 4 threads, and an additional
increase of evaluations/second up to 8 threads. However, since smaller
population brings about decreased diversity, more evaluations were
needed and time actually increased from 4 threads up.


%----------------------------------------------------------------
\section{Conclusions}
\label{sec:conclusions}

In this paper we explored the parameter space in a concurrent evolutionary algorithm
looking for the combination that yields the best speedup performance, without affecting 
its algorithmic effectiveness.
In order to do so, the size of messages interchanged within the channel
has been redesigned using the distribution of probabilities as a representation of the
population.
Different messaging strategies has been also tested.

Experiments show that the number of generations that the population undergoes must be kept to a small number.
The results also indicate that the EDA strategy is the fastest, with a relatively low impact in the number of evaluations, as the messages are the most compact.
Finally, obtained results have shown that simultaneous threads running 
an evolutionary algorithm via population splitting do increase the number of simultaneous evaluations,
leading the new concurrent evolutionary algorithm to improve the
performance in comparison to the equivalent single thread evolutionary
algorithm. However, the total time does not decrease in the same
proportion due to the effect of the division of population.

For the time being, it has been proved that these concurrent
algorithms are able to improve performance in a limited way. Further
explorations into the parameter space will be needed, including
adaptivity of other parameters or use of a random parametrization or
other communication strategies.


%----------------------------------------------------------------
\begin{acks}
This paper has been supported in part by
projects DeepBio (TIN2017-85727-C4-2-P) and TecNM Project 5654.19-P. 
\end{acks}


%----------------------------------------------------------------
\bibliographystyle{ACM-Reference-Format}
\bibliography{../geneura,../concurrent,../perl6} 

\end{document}
