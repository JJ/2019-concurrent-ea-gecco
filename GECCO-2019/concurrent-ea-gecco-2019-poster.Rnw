%\documentclass[sigconf, authordraft]{acmart}
\documentclass[sigconf]{acmart}

\usepackage{booktabs} % For formal tables
\usepackage{tikz}
\usepackage{pgfbaselayers}
\usepackage[underline=false]{pgf-umlsd}
\usepackage{wrapfig}
\usetikzlibrary{shadows}
\usetikzlibrary{arrows}

% Copyright
%\setcopyright{none}
%\setcopyright{acmcopyright}
%\setcopyright{acmlicensed}
\setcopyright{rightsretained}
%\setcopyright{usgov}
%\setcopyright{usgovmixed}
%\setcopyright{cagov}
%\setcopyright{cagovmixed}


% DOI
\acmDOI{10.1145/nnnnnnn.nnnnnnn}

% ISBN
\acmISBN{978-x-xxxx-xxxx-x/YY/MM}

% Conference
\acmConference[GECCO '19]{the Genetic and Evolutionary Computation Conference 2019}{July 13--17, 2019}{Prague, Czech Republic}
\acmYear{2019}
\copyrightyear{2019}

%\acmArticle{4}
\acmPrice{15.00}

% These commands are optional
%\acmBooktitle{Transactions of the ACM Woodstock conference}
%\editor{Jennifer B. Sartor}
%\editor{Theo D'Hondt}
%\editor{Wolfgang De Meuter}

\hypersetup{draft} 
\begin{document}

<<setup, cache=FALSE,echo=FALSE>>=
library(ggplot2)
library(ggthemes)

data.freqs.nw.gens <- read.csv('../data/gecco-2019-freqs-noweb-generations.csv')
data.compress.nw.gens <- read.csv('../data/gecco-2019-compress-noweb-generations.csv')
data.freqs.generations <- read.csv('../data/gecco-2019-freqs-generations.csv')
data.compress.generations <- read.csv('../data/gecco-2019-compress-generations.csv')
data.evostar <- read.csv("../data/evostar2019.csv")

data.generations <- data.frame(
    strategy=c(rep("Compress",length(data.compress.generations$Generation.Gap)),
               rep("CompressNW",length(data.compress.nw.gens$Generation.Gap)),
               rep("EDA",length(data.freqs.generations$Generation.Gap)),
               rep("EDANW",length(data.freqs.nw.gens$Generation.Gap)),
               rep("Full",length(data.evostar$gap))),
    gap=c(data.compress.generations$Generation.Gap,data.compress.nw.gens$Generation.Gap,data.freqs.generations$Generation.Gap,data.freqs.nw.gens$Generation.Gap,data.evostar$gap),
    evaluations=c(data.compress.generations$Evaluations,data.compress.nw.gens$Evaluations,data.freqs.generations$Evaluations,data.freqs.nw.gens$Evaluation,data.evostar$evaluations),
    time=c(data.compress.generations$Time,data.compress.nw.gens$Time,data.freqs.generations$Time,data.freqs.nw.gens$Time,data.evostar$time) )

@

%----------------------------------------------------------------
\title{Improving the algorithmic efficiency and performance of channel-based evolutionary algorithms}
% Same title as EvoApp paper? - Juanlu
% I hadn't realized that... 
% I propose that the subtitle becomes the title and the subtitle becomes something like: "The case of a Perl6-based implementation" -  Juanlu

  \author{Juan-Juli\'an Merelo Guerv\'os}
%  \orcid{1234-5678-9012}
  \affiliation{%
    \institution{Universidad de Granada/CITIC}
    \city{Granada,Spain}
    \postcode{18071}
  }
  \email{jjmerelo@gmail.com}
  
  \author{Juan Luis Jim\'enez Laredo}
  \affiliation{%
    \institution{Ri2C-LITIS, Universit\'e Le Havre}
    \city{Le Havre, France}
  }
  \email{juanlu.jimenez@univ-lehavre.fr}

  \author{Pedro A. Castillo}
  \affiliation{%
    \institution{Universidad de Granada/CITIC}
    \city{Granada,Spain}
    \postcode{18071}
  }
  \email{pacv@ugr.es}

  \author{Mario Garc\'ia Valdez}
  \affiliation{%
    \institution{Tecnol\'ogico Nacional de M\'exico}
    \city{Tijuana}
    \state{M\'exico}
    \postcode{22414}
  }
  \email{mario@tectijuana.edu.mx}

  \author{Sergio Rojas-Galeano}
  \affiliation{%
    \institution{{\small Universidad Distrital Francisco Jos\'e de Caldas}}
    \city{Bogot\'a}
    \state{Colombia}
    \postcode{111311}
  }
  \email{srojas@udistrital.edu.co}
  
  % The default list of authors is too long for headers.
  \renewcommand{\shortauthors}{J.J. Merelo Guerv\'os et al.}

  
%----------------------------------------------------------------
\begin{abstract}
Concurrent evolutionary algorithms use threads that communicate via
messages. Parametrizing the work in every thread and the
way they communicate results is a major challenge in its design. In
this paper we work with concurrent evolutionary algorithms implemented
in Perl 6, and explore different options of single-thread evolution
parameterization, communication and mixing of results, showing that 
scalability is achieved in a multi-core environment.

\end{abstract}

%
% The code below should be generated by the tool at
% http://dl.acm.org/ccs.cfm
% Please copy and paste the code instead of the example below. 
%
 \begin{CCSXML}
<ccs2012>
<concept>
<concept_id>10003752.10003809.10011778</concept_id>
<concept_desc>Theory of computation~Concurrent algorithms</concept_desc>
<concept_significance>500</concept_significance>
</concept>
<concept>
<concept_id>10010147.10010257.10010293.10011809.10011812</concept_id>
<concept_desc>Computing methodologies~Genetic algorithms</concept_desc>
<concept_significance>500</concept_significance>
</concept>
</ccs2012>

\end{CCSXML}

\ccsdesc[500]{Theory of computation~Concurrent algorithms}
\ccsdesc[500]{Computing methodologies~Genetic algorithms}

\keywords{Concurrent evolutionary algorithms, performance evaluation}


\maketitle

%----------------------------------------------------------------
\section{Introduction}

Nowadays, concurrent programming is the best option to leverage the number of processes and threads that a multi-core processor architecture can host.

%% where many processes 
%% and threads can coexists at the same time.
%% This eventually means that many processes (heavy or lightweight) 
%% can be leveraged to take full advantage of the processor capabilities.

These capabilities must be matched at an abstract level by concurrent languages
that incorporate programming constructs intended to manage creation, execution 
and termination of processes, as well as new models of communication between such processes.
%These capabilities must be matched at an abstract level by concurrent languages 
%that are characterized by the presence of programming constructs that manage processes.
%The language provides an abstraction for communication using new communication constructs.
%that build on them so that high-level algorithms can use them without
%worrying about the low-level mechanisms of creation or destruction of
%threads, or how data is shared or communicated among them. These
%languages are called concurrent, and the programming paradigm
%implemented in them concurrency-oriented programming or simply
%concurrent programming \cite{Armstrong2003}. 
%These languages are characterized by the presence of
%programming constructs that manage processes like first class
%objects; that means that the language includes  operators for acting upon them and the
%possibility of using them like parameters or function's result
%values. This changes the coding of concurrent algorithms due to the
%direct mapping between patterns of communications and processes with
%language expressions; on the one hand it becomes simpler since the
%language provides an abstraction for communication, on the other hand
%it changes the paradigm for implementing algorithms, since these new
%communication constructs have to be taken into account. 
Moreover, concurrent programming adds a layer of abstraction over the parallel
facilities of processors and operating systems, offering a
high-level interface that allows the user to program modules of code to
be executed in parallel threads \cite{andrews1991concurrent}.
Different languages offer different concurrency policies depending on how they deal with state, 
that is, data structures that could be accessed from several processes. They can be divided roughly between channel-based concurrency, with no shared nor stored state, and actor based concurrency, which stores state (in actors) but does not share it. This last model is the one used by the Perl 6 language, which is the one we are going to be using in this work.

Previously we
designed an evolutionary algorithm based on using a stateless architecture  \cite{Merelo:2018:MEA:3205651.3208317,merelo:WEA}, 
with different processes reacting to a channel input without changing state, and writing to the channel,
and explored its possibilities. However, the design of this kind of stateless algorithm leaves many options open, and they have to be explored heuristically. First, we will find out what are the best parameters from the point of view of the algorithm; then, we will test several communication strategies: 
a lossless one that compresses the population, and a lossy one 
that sends a representation of population gene-wise statistics.

%----------------------------------------------------------------
\section{Experimental setup and results}
\label{sec:exp}

Building upon the design we used in previous experiments\cite{Merelo:2018:MEA:3205651.3208317}, 
here our goal was to
%The baseline we are coming from is similar to the one used in previous experiments
%\cite{Merelo:2018:MEA:3205651.3208317}. Our intention was to
create a system that was not functionally equivalent to a sequential
evolutionary algorithms, and that followed the principle of
communicating sequential processes. 
%In this kind of methodology, we
%will have processes (or threads) communicating state through
%channels. Every process itself will be stateless, reacting to the
%presence of messages in the channels it is listening to and sending
%result back to them, without changing state.
As in the previous papers, \cite{merelo:WEA}, we will use two
groups of threads and two channels:  %. We will see them in turns.

%The two groups of threads perform the following functions:

\begin{itemize}
\item The {\em evolutionary} threads will be the ones that will be in
  principle running the evolutionary algorithm.
\item The {\em mixing} threads will {\em mix} populations, and create
  new ones as a mixture of them.

%\end{itemize}
%
%The two channels carry messages that are equivalent to populations, but they do so in a different way:
%\begin{itemize}
  
\item The {\em evolutionary} channel will be used for carrying
  non-evolved, or generated, populations.
\item The {\em mixer} channel will carry, {\em in pairs}, evolved
  populations. 
\end{itemize}

The proposed system is illustrated in Figure \ref{fig:schematic}.
%These will be connected as shown in Figure \ref{fig:schematic}.
The evolutionary group of threads will read only from the evolutionary channel,
evolve for a number of generations, and place result in the mixer
channel; the mixer group of threads will read only from the mixer
channel, in pairs. From every pair, a random element is put back into
the mixer channel, and a new population is obtained and sent back to
the evolutionary channel. 
The aim of using two channels is
to avoid deadlocks; the fact that one population is always written 
back to the mixer channel avoids starvation in that channel. 
%How this runs in practice is shown in Figure \ref{fig:schematic}, where the
%timeline of the interchange of messages between the evolver and mixer
%threads and evolver and mixer channels is clarified.

%One of the problems of the baseline configuration was that
%communication took a great amount of time, adding some overhead to the
%algorithm. The {\em message} consisted of the whole population, and
%the size increased with population size, obviously. This tipped the
%balance between communication and computation towards communication,
%so that the more threads, the more communication was taking place. Our
%first intention in this paper was to slim down messages so that they
%took less bandwidth (or memory) and less time to send and process. In
%general, this strategy also can be framed in the context of migration
%strategies, since that is the most similar thing in the context of
%parallel algorithms. In parallel algorithms, an adequate selection of
%migration strategies, balancing exploration and exploitation, is the
%key to achieving high performance, as indicated in
%\cite{Cantu-Paz:1999:MPT:2933923.2934003}.
%In the context of concurrent evolutionary algorithms we will talk
%about {\em population messages}, but their effect is going to be similar. 

In the baseline configuration the communication model was costly in time, since the {\em message} consisted of the entire population whose memory size increases with larger populations.
We observe that this strategy is reminiscent of some migration policies used in other models of parallel evolutionary algorithms.

% In the baseline configuration the communication took a great amount of time. 
% The {\em message} consisted of the whole population and the size increased with population size.
% This strategy also can be framed in the context of migration strategies, since that is the most similar thing in the context of parallel algorithms.

%\begin{figure}[tb]
\begin{wrapfigure}[29]{r}{0.45\columnwidth}
  %\centering
  \vspace{-.5\intextsep}
\hspace*{-.95\columnsep}
\scalebox{.8}[.6]{
\begin{sequencediagram}

\newthread[red]{E}{Evolver} 

\tikzstyle{inststyle}+=[rounded corners=3mm] 
\newinst{C}{Channel}

\tikzstyle{inststyle}+=[rounded corners=0]
\newthread[blue]{M}{Mixer}

\begin{call}{E}{evolve()}{E}{}\end{call}

\setthreadbias{east}
\begin{messcall}{E}{$pop_1$}{C} 
\mess{C}{$pop_1$}{M}
\end{messcall}

\prelevel\prelevel
\begin{call}{E}{evolve()}{E}{}\end{call}

\setthreadbias{east}
\begin{messcall}{E}{$pop_2$}{C}  
\mess{C}{$pop_2$}{M}
\end{messcall}

\prelevel\prelevel
\begin{call}{M}{mix()}{M}{}\end{call}

\postlevel\postlevel
\setthreadbias{west}
\begin{messcall}{M}{\shortstack{ \{\ $mixpop_1$,\\ $mixpop_2$,\\ \vdots \\ $mixpop_k$ \} }}{C}

\mess{C}{$mixpop_1$}{E} 
\begin{call}{E}{evolve()}{E}{}\end{call}

\setthreadbias{east}
\begin{messcall}{E}{$pop_3$}{C} 
\postlevel
\mess{C}{$mixpop_2$}{E} 
%\prelevel
\mess{C}{$pop_3$}{M}
\end{messcall}

\prelevel\prelevel
\begin{call}{E}{evolve()}{E}{}\end{call}

\setthreadbias{east}
\begin{messcall}{E}{$pop_4$}{C}
\mess{C}{$pop_4$}{M}
\end{messcall}

\setthreadbias{west}
\prelevel
\mess{C}{$mixpop_k$}{E}%\end{messcall}

\end{messcall}

\prelevel\prelevel
\begin{call}{M}{mix()}{M}{\vdots}\end{call}
\prelevel
\begin{call}{E}{evolve()}{E}{\vdots}\end{call}
\vspace{-.2cm}
\end{sequencediagram}
}
\caption{Schematic of communication between threads and channels for
  concurrent EAs. 
%  The two central bars represent the channel, and
%  color corresponds to the {\em main} function they perform; blue for
%  mixer, red for evolver. As the figure shows, the evolver threads
%  always read from the mixer channel, and always write to the evolver channel.
  }
\label{fig:schematic}
\end{wrapfigure}
%\end{figure}

In this paper we introduce two different messaging strategies:  
{\em EDA}, in which the message consist of a probability distribution of each gene in the population,
and {\em compress}, that simply bit-packs the population without the fitness into a message, using 1 bit per individual.

%\begin{itemize}
%\item One we have called {\em EDA}, or estimation of distribution
%  algorithm, whose basic idea is that the population message will
%  contain the probability distribution over each gene. In this sense,
%  this strategy is similar to the one presented by de la Ossa et
%  al. in \cite{10.1007/978-3-540-30217-9_25}. Not being an estimation
%  of distribution algorithm {\em per se}, since the evolutionary
%  thread runs a canonical genetic algorithm, when the message is being
%  composed, every (binary) gene of the 25\% best individuals in the
%  population is examined, and an array with the
%  probabilities for each gene is sent to the mixer thread. The {\em
%    mixer} thread, in turn, just takes randomly one probability from
%  each of the two {\em populations} (actually, distributions), instead
%  of working on individuals. While in the baseline strategy the
%  selection took place in the mixer thread, that eliminated half the
%  population, in this case the selection takes place when composing
%  the message, since just the 25\% best individuals are selected to
%  compute the probability distribution of genes. When the evolver
%  thread reads the message, it rebuilds the population based on this
%  distribution.
%\item The second is called {\em compress}, and it simply bit-packs the
%  population, without the fitness, into a message which uses 1 bit per
%  individual, and then 64 bits, or simply 8 bytes, to transmit a
%  single individual in the population. This strategy is equivalent to
%  the baseline, except it introduces an additional step of evaluating
%  the population when mixing and receiving it from the evolutionary
%  channel. It is hoped that this additional evaluation overhead does
%  compensates the communication overhead that is eliminated.
%\end{itemize}


%In the same way we did in our previous papers, first we will have to
%evaluate these new strategies compared with baseline; since the
%overhead will be different depending on the computation time, which in
%this case is regulated by the number of generations that are going to
%be performed by every thread, we will first perform an experiment
%changing that. We will call this parameter the {\em generation gap},
%implying that's the gap between receiving a message and activating the
%thread and sending it, deactivating it. 
%Besides, what we want to find out in these set of experiments is what is the 
%generation gap that gives the best performance in terms of raw time to
%find a solution, as well as the best number of evaluations per second.

Similarly to our previous experiments, first we will compare 
these new strategies with respect to a baseline, 
evaluating the gap between receiving a message and activating the thread until sending the message to deactivate it.
Besides, we heuristically studied other messaging
strategy called {\em no writeback} ({\sf nw}), where the mixer
thread sends the individuals to the evolver channel, to undergo
an additional round of evolution.

%Additionally, it is impossible to know from first principles if this
%setup is the only possible. We have to heuristically explore other
%possibilities; in this case, we will explore another messaging
%strategy called {\em no writeback}, or {\sf nw}, where the mixer
%thread, instead of sending one of the individuals back again to the
%mixer thread, sends it to the evolver channel, where it will undergo
%an additional round of evolution. The main difference between these
%two strategies is twofold:\begin{itemize}
%\item The mixer channel can be empty for some time, since it is not
%  always holding at least one population message (written back every
%  time it is activated). This might lead to {\em starvation} of the
%  mixer thread, but in fact it will not take long since it is going to
%  be processed immediately by the evolver thread.
%\item Every population is mixed just once, which might lead to
%  improvements in the algorithm.
%\end{itemize}


The experiments have been prepared by using OneMax function with 64
bits. This function was chosen primarily since it was the one used in
previous experiments, but also because it is a classical benchmark, it
can be easily programmed in Perl 6, and allowed us to focus in the 
design of the relevant mechanisms of the concurrent evolutionary model. 
We used the open source {\tt Algorithm::Evolutionary::Simple}
Perl 6 module. % for the evolutionary part, and wrote the different
%scripts in the same language. The latest version, compiled from
%source, of Perl 6 was used, and experiments were performed in a
%Intel(R) Core(TM) i7-4770 CPU at 3.40GHz running Ubuntu 14.04 server.
%All scripts have a free license and have been released in GitHub,
%where the data generated by every single experiment is also hosted.

The parameters used are shown in Table
\ref{tab:gens}. Two evolver threads are needed, as mentioned above, to avoid
starvation of the mixer thread. The generation gap was checked for
the shown values, although in some cases we extended it to 4
and 64 generations. The population was sized as in previous papers using
the bisection method, and the number of initial populations created
and sent to the evolver channel was also designed to avoid starvation.
%that way, as soon as the first two populations are evaluated
%simultaneously by the two threads and sent to the mixer channel, this
%channel will always hold a population that will combine with a fresh
%one coming from either of the mixer threads.

\begin{table}[b!]
  \centering
  \caption{Parameters used to explore the generation gap}
  \footnotesize
  \label{tab:gens}
  \begin{tabular}{|lr|lr|}
    \hline
    Parameter & Value & Parameter & Value\\
    \hline
Evolver threads & 2    &  Population size & 256 \\
Mixer threads & 1      &  Initial populations & 3 \\
Generations & 8,16,32  &  Bits & 64   \\
Repetitions & $>=$ 15  &       &      \\
  \hline
  \end{tabular}
\end{table}

First, the generation gap and strategy that obtains the smaller
number of evaluations has been found.
%As shown in Figure \ref{fig:threads2}, 
In general, the number of evaluations increases with the generation
gap. More evolution without interchange with other populations implies
more exploitation, and then the possibility of stagnation.
The lowest number of evaluations is achieved for the EDA strategy with no writeback.

%Since we are exploring the parameter space, we will first try
%to find out the generation gap and strategy that obtains the smaller
%number of evaluations, indicating it is the best algorithmic
%strategy. These are shown in Figure \ref{fig:evals}, where the two
%messaging strategies, EDA and compress, with or without writeback, are
%plotted and compared with the baseline strategy, which uses the full,
%evaluated population as a message. The first observation is that, in
%general, the number of evaluations increases with the generation
%gap. More evolution without interchange with other populations implies
%more exploitation, and then the possibility of stagnation. For 8
%generations the results are very similar, but they diverge with the
%generation gap. Clearly, the baseline strategy achieves the lowest
%number of evaluations, for two reasons: it does not need to
%re-evaluate the population when the message is received, but also, in
%the case of EDA, the population is rebuilt from its statistical
%description, so the exact individual (with the possible highest
%fitness) is not kept. The lowest number of evaluations, although not
%significantly so, is achieved for the EDA strategy with no writeback.
%This chart can also be used to check the performance of the {\em no-writeback} 
%strategies. In principle, since they use more evolved
%populations, they should be better. As a matter of fact, since they
%are injecting an additional population that is actually evolved, they
%use more evaluations. So in principle these strategies will be
%discarded.
Additionally, the raw performance in evaluations per second of all strategies has been evaluated, 
showing that the EDA strategy is superior to the rest for all generation gaps. 
%Additionally, we evaluate the raw performance, in evaluations per
%second, of all strategies, since at the end of the day, working with
%concurrent evolutionary algorithms pursue higher speed. This is shown
%in Figure \ref{fig:evals2}, which shows in the $y$ axis the number of
%evaluations per second. In this case, the EDA strategy is clearly
%superior to the rest, beating them significantly for all generation
%gaps. In this case, generation gap == 8 will be chosen, since it
%achieves the best combination of algorithmic and wallclock
%performance.
As a result, the EDA-style communication strategy improves the speed of the algorithm, mainly due to the compactness of its messages.

%Our initial intention in this paper was to design a communication
%strategy that improves the speed of the algorithm, and with the EDA
%strategy we have achieved just that. However, there are two parts in
%this strategy: using significantly smaller messages, and moving
%selection strategy from the mixer to the evolver (when computing the
%probability distribution). In fact, 1/4 of the population is used to
%compute the distribution, as opposed to the baseline mixer (labeled
%``Full''), which takes the better half of the pair of populations for
%mixing. This is undoubtedly a factor that contributes to balance the
%number of additional evaluations needed for the new, rebuilt,
%populations. However, the increase of speed must be entirely due to
%the compactness of the messages used by this strategy.

%However, the intention of concurrent evolutionary algorithms is to
%leverage the power of all threads and processors in a computer, so
%unlike in previous papers, we must find a version of the algorithm
%that speeds up with the number of threads.
%After many tests, eventually the scaling strategy was simply to divide
%the total population by the number of threads.

%However, the intention of concurrent evolutionary algorithms is to
%leverage the power of all threads and processors in a computer, so
%unlike in previous papers, we must find a version of the algorithm
%that speeds up with the number of threads. We will settle on the
%communication strategy, EDA, that has been proved to work before, and
%the population size (256) and generation gap (8) that has been proved
%to work previously. The number of initial populations was set to the
%number of threads plus one, as the minimum required to avoid starvation. 
%However, we need to devise a strategy that actually makes scaling possible and profitable.
%After many tests, eventually the scaling strategy was simply to divide
%the total population by the number of threads. Initially we had two
%threads and total population equal to 512, so our strategy, called AP
%for adaptive population, was to divide the number of total individuals
%between the threads, so that 4 threads, for instance, get 128
%individuals each. We repeated every run 15 times.

%In this case, experiments were run on a different machine with the
%Ubuntu 18.04 OS and an AMD Ryzen 7 2700X Eight-Core Processor
%at 3.7GHz. Figure \ref{fig:screenshot} shows the utility htop with an
%experiment running; the top of the screen shows the rate at which all
%cores are working, showing all of them occupied; of course, the
%program was not running exclusively, but the list of processes below
%show how the program is replicated in several processor, thus
%leveraging their full power. Please check also the number of threads
%that are actually running at the same time, a few of which are being used by our application. 

%\begin{figure}[h!tb]
%  \centering
%<<threads2, cache=FALSE,echo=FALSE,fig.height=4,message=FALSE>>=
%ggplot(data.freqs.ap.threads,aes(x=Threads,y=Time,group=Threads))+geom_boxplot(fill='blue',notch=TRUE)+theme_tufte()+scale_x_continuous(name="Threads",breaks=c(2,4,6,8))+labs(x="Generation.Gap",y="Time",title="Scaling")
%@ 
%\caption{Total time vs. number of threads. Lower is better.}
%\label{fig:threads2}
%\end{figure}

%We are first interested in the number of evaluations needed to find
%the solution, which are plotted in Fig. \ref{fig:threads1}. It
%increases slightly and not significantly from 2 to 4 threads, but it
%does increase significantly for 6 and 8 threads, indicating that the
%algorithm is performing significantly worse when we increase the number
%of threads. This is probably due to the fact that we are
%simultaneously decreasing the population size, leading to earlier
%convergence for the number of generations (8) it is being used. This
%interplay between the degree of concurrency, the population size and
%the number of generations will have to be explored further.

%But we were also interested in the actual wallclock time, plotted in
%Fig. \ref{fig:threads2}. The picture shows that it decreases
%significantly when we go from 2 (the baseline) to 4 threads, since we
%are using more computing power for (roughly) the same number of
%evaluations. It then increases slightly we we increase the number of
%threads; as a matter of fact and as shown in \ref{fig:threads3}, the
%number of evaluations per second increases steeply up to 6 threads,
%and slightly when we use 8 threads. However, the amount of evaluations
%needed overcompensates this speed, resulting in a worse results. It
%confirms, however, that we are actually using all threads for
%evaluations, and if only we could find a strategy that didn't need
%more evaluations we should be able to get a big boost in computation
%time that scales gracefully to a high number of processors.


%----------------------------------------------------------------
\section{Conclusions}
\label{sec:conclusions}

In this paper we explored the parameter space in a concurrent evolutionary algorithm
looking for the combination that yields the best speedup performance, without affecting 
its algorithmic effectiveness.
In order to do so, the size of messages interchanged within the channel
has been redesigned using the distribution of probabilities as a representation of the
population.
Different messaging strategies has been also tested.

Experiments show that the number of generations that the population undergoes must be kept to a small number.
The results also indicate that the EDA strategy is the fastest, with a relatively low impact in the number of evaluations, as the messages are the most compact.
Finally, obtained results have shown that simultaneous threads running 
an evolutionary algorithm via population splitting do increase the number of simultaneous evaluations,
leading the new concurrent evolutionary algorithm to improve the performance in comparison to the equivalent single thread evolutionary algorithm.


%----------------------------------------------------------------
\begin{acks}
This paper has been supported in part by
\href{http://geneura.wordpress.com}{GeNeura Team}, 
projects DeepBio (TIN2017-85727-C4-2-P) and TecNM Project 5654.19-P. 
\end{acks}


%----------------------------------------------------------------
\bibliographystyle{ACM-Reference-Format}
\bibliography{../geneura,../concurrent,../perl6} 

\end{document}
